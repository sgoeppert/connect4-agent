\section{Implementierung der Neuronalen Netze}
\label{chap:nn-impl}

Das Finden der richtigen Architektur des neuronalen Netzes ist eine Kunst.
Es gibt unzählige Kombinationsmöglichkeiten für ein einfaches neuronales Netz.
Auch wenn nur vollständig verbundene Schichten verwendet wrden, so kann die Anzahl der Schichten, die Menge an Einheiten pro Schicht, die Aktivierungsfunktion, die Lernrate, der Optimierungsalgorithmus und mehr variiert werden.
Alle möglichen Kombinationen zu erforschen, ist nicht möglich und auf eigene Erfahrungswerte kann ich mich auch nicht stützen.
Andere Arbeiten, die neuronale Netze benutzen um Spiele zu lernen, wie AlphaGo, können zur Orientierung benutzt werden.
Die in AlphaGo verwendete Architektur ist allerdings sehr komplex.

Durch Werkzeuge, wie das an der Texas A \& M University entwickelte Auto-Keras, kann die Suche nach einer geeigneten Architektur automatisiert werden.\info{Quelle Auto-Keras}
Der Auto-Keras Suchalgorithmus erforscht einen vordefinierten Hyperparameter-Raum von neuronalen Netzen, und sucht nach dem Netzwerk, das bei gegebenen Trainingsdaten eine Kostenfunktion minimiert.
Die Struktur des Netzwerkes kann dabei grob mit Auto-Keras-Einheiten festgelegt werden.
Die Netzwerke, die ich für ``Vier Gewinnt'' betrachten will, bestehen aus ``Dense''-Einheiten und Conv2D-Einheiten.
Jeder dieser Auto-Keras-Einheiten besteht aus 1 bis 3 Blöcken von Netzwerk-Schichten.
Die Dense-Einheit besteht aus bis zu 3 Blöcken mit jeweils einer vollständig verbundenen Schicht so wie einer optionalen Batch-Normalisierungs und einer optionalen Dropout Schicht.
Conv2D-Einheiten bestehen aus Blöcken mit je 1 oder 2 Conv2D-Schichten, einer optionalen Max-Pooling Schicht und einer Dropout Schicht.
Die Aktivierungsfunktion der versteckten Schichten ist durchweg die ReLU-Funktion, die Aktivierungsfunktion der letzten Schicht hängt von der Aufgabe ab, ist in diesem Anwendungsfall aber einfach linear.

Die Batch-Normalisierung und Dropout Schichten dienen der Regularisierung des neuronalen Netzes.
Damit kann das Overfitting der Trainingsdaten verhindert werden.

\info{Regularisierung sollte in NN-Theorie erklärt werden}
\info{Grafik mit Block-Architektur erstellen und einbinden}

\subsection{Die Trainingsdaten}

Die Suche nach guten Netzwerkarchitekturen geschieht automatisch, die Lernfähigkeit hängt aber von der Repräsentation des Spielfeldes ab.
Das vorhandene Array mit 0 für leere Felder und 1,2 für die Steine der jeweiligen Spieler ist für ein Netzwerk nur schwer erlernbar.
Da ein Stein von Spieler 2 nicht doppelt so gut wie einer von Spieler 1 ist, ergibt diese Repräsentation nur wenig Sinn.
Besser ist es dagegen, wenn einer der Spieler als -1 dargestellt wird.
Vermutlich hilft der Gegensatz dabei, Muster zu lernen.
Eine dritte Repräsentation benutzt ein zwei-Dimensionales Array für jeden Spieler und ein zusätzliches Array gefüllt mit 0, wenn Spieler 1 am Zug ist und 1 wenn Spieler 2 am Zug ist.

\info{Grafik der verschiedenen Darstellungen einbringen}

Die ersten beiden Darstellungen werden, aufgrund ihrere ``flachen'' Form, mit einem dichten Netz verwendet, während die dritte Darstellung sich durch ihre zwei-Dimensionale Form mit 3 ``Kanälen'' sehr gut für ein Convolutional Network eignet.


Die Trainingsdaten wurden durch Selfplay generiert.
Zwei MCTS-Spieler haben 16000 Spiele gegeneinander gespielt.
Nach jedem Spiel wurden alle Spielzustände mit der Belohnung des Besitzers dieses Zustandes versehen.
Hat der Besitzer gewonnen, so ist der Wert des Zustands 1, hat er verloren ist er -1 und Unentschieden ist 0.
Die Zustände 1, 3, 5, 7, \ldots gehören Spieler 1, die Zustände 2, 4, 6, \ldots gehören Spieler 2.
Durch die 16000 Spiele wurden ingesamt 400000 Zustände erzeugt.
Dieser Datensatz wurde augmentiert, indem die horizontale Spiegelung jedes Zustandes hinzugefügt wurde.

Doppelte Zustände wurden zusammengefügt indem der Durchschnitt ihrer Zielwerte gebildet wurde.
Hat ein Zustand zum Beispiel zwei Mal zu einem Sieg und ein Mal zu einer Niederlage geführt, so ist das Trainingsziel für diesen Zustand $\frac{1+1-1}{3} = 0.3\overline{3}$.
Wenn Duplikate nicht reduziert werden, sollte das Netzwerk dennoch den durchschnittlichen Wert dieser Zustände lernen können, ein vorheriges entfernen von Duplikaten macht es aber hoffentlich leichter.

Als zusätzliches Experiment habe ich den Zielwert der Zustände auf den Bereich $[0,1]$ normalisiert.
Effektiv suche ich also nach den besten Netzwerk-Architekturen für drei verschieden Repräsentationen des Spielfeldes mit normalisierten und nicht normalisierten Zielwerten.

\subsection{Trainingsergebnisse}
Da es sich bei diesem Problem um ein Regressionsproblem handelt, habe ich als Kostenfunktion den Mean Squared Error verwendet und betrachte zusätzlich den Mean Absolute Error.

\info{Tensorboard Graph}

Beim Training der Netzwerke habe ich festgestellt, dass sich die erste Repräsentation (0 = leer, 1 = Spieler 1, 2 = Spieler 2) nicht für neuronale Netze eignet.
\info{Datensatz analysieren, wie sind die Trainingsziele verteilt? Verhältnis von nahe 0 zu 1 oder -1}
Die erste Repräsentation erreicht beim Training nur einen MSE von 0.8.
Bei einer Verteilung der Trainingsziele von 20\%, 40\%, 40\% für 0, 1 und -1, kann ein MSE von 0.8 bereits erreicht werden, indem jeder Wert als 0 vorhergesagt wird.

Die anderen Repräsentation haben allerdings einen sehr guten Trainingswert erreicht.
Generell ist der erzielte Fehler, wenn die Zielwerte auf 0\textendash1 normalisiert sind, geringer, dies liegt aber generell an dem reduzierten Wertebereich.
Das dichte Netzwerk hat, bei normalisierten Werten, einen MSE von 0.0237 bei einem MAE von 0.11 erzielt.
Im Schnitt lag die Vorhersage also um 0.11 daneben (wenn der korrekte Wert 0.6 war, hat das Netzwerk z.B.\ 0.49 vorhergesagt).
Ohne Normalisierung erreicht das beste dichte Netzwerk einen MSE von XXX bei MAE YYY.

Das ConvNet dagegen hat deutlich besser abgeschnitten.
Der Fehler bei Normalisierung ist ähnlich mit 0.034, ohne Normalisierung erreicht das ConvNet aber einen sehr guten MSE von 0.14 bei MAE 0.11.

\subsection{Kombination von Neuronalem Netz mit MCTS}
Der Zweck des neuronalen Netzes ist die Evaluation der Spielzustände in der Simulationsphase zu verbessern.
Durch das vorherige Lernen von echten Werten, kann das Netzwerk präzisere Vorhersagen über den Wert eines Zustands liefern, als durch zufällige Simulationen möglich ist.
Die Kombination des neuronalen Netzes mit der Baumsuche ist trivial.
Ein NNEvaluator führt weiterhin die zufällige Simulation des Zustandes aus, lässt aber das Netzwerk den Spielzustand und den gespiegelten Zustand vom Netzwerk evaluieren.
Der Durchschnitt der Vorhersagen des Netzwerks wird mit einem Gewichtungsfaktor $\alpha$ mit dem Ergebnis der Simulation kombiniert.
Das Ergebnis der Evaluation ist dann \texttt{prediction * alpha + (1 - alpha) * simulation}.

\info[inline]{Virtual Loss kann raus, wird doch nicht benutzt. Eher was für den Ausblick - Parallelisierung generell was für Ausblick.}
Da die sequenzielle Verarbeitung von Spielzuständen durch das Netzwerk sehr langsam ist, kann die Baumsuche parallelisiert werden, um die Grafikkarte besser auszureizen.
Die möglichen Arten der Parallelisierung sind die Wurzel-Parallelisierung, bei der jeder Prozess oder Thread eine eigene Kopie des Baumes erstellt, welche in regelmäßigen Abständen zusammengefügt werden, und die Baum-Parallelisierung, bei der sich die Prozesse den Baum teilen.
\info{Grafik der Parallelisierungen, Quelle Parallelisierung}
Damit die verschiedenen Ausführungszweige der Baum-Parallelisierung unterschiedliche Zweige im Baum erforschen, wird ein ``virtueller Verlust'' eingeführt.
Wenn ein Prozess einen Knoten besucht, wird die Anzahl der Besuche $N(s)$ um einen festen Wert erhöht und diese Besuche temporär als Niederlagen gewertet.
In der Update-Phase werden die zusätzlichen Besuche dann wieder entfernt und der Wert um die tatsächliche Bewertung ergänzt.
Dadurch werden andere Prozesse von diesem Knoten weggelenkt und wählen einen anderen Pfad durch den Baum.
\info{Grafik des virtuellen Verlustes, Quelle Virtual Loss}