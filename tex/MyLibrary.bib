
@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	language = {en},
	number = {4},
	urldate = {2020-07-30},
	journal = {The bulletin of mathematical biophysics},
	author = {McCulloch, Warren S. and Pitts, Walter},
	month = dec,
	year = {1943},
	pages = {115--133},
	file = {Springer Full Text PDF:/home/shadex91/Zotero/storage/FVVXRNWX/McCulloch and Pitts - 1943 - A logical calculus of the ideas immanent in nervou.pdf:application/pdf}
}

@book{sutton_reinforcement_2018,
	address = {Cambridge, Massachusetts},
	edition = {Second edition},
	series = {Adaptive computation and machine learning series},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	language = {en},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {2018},
	keywords = {Reinforcement learning},
	file = {Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf:/home/shadex91/Zotero/storage/S3Y38LPA/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf:application/pdf}
}

@article{kocsis_improved_nodate,
	title = {Improved {Monte}-{Carlo} {Search}},
	abstract = {Monte-Carlo search has been successful in many non-deterministic games, and recently in deterministic games with high branching factor. One of the drawbacks of the current approaches is that even if the iterative process would last for a very long time, the selected move does not necessarily converge to a game-theoretic optimal one. In this paper we introduce a new algorithm, UCT, which extends a bandit algorithm for Monte-Carlo search. It is proven that the probability that the algorithm selects the correct move converges to 1. Moreover it is shown empirically that the algorithm converges rather fast even in comparison with alpha-beta search. Experiments in Amazons and Clobber indicate that the UCT algorithm outperforms considerably a plain Monte-Carlo version, and it is competitive against alpha-beta based game programs.},
	language = {en},
	author = {Kocsis, Levente and Szepesvari, Csaba and Willemson, Jan},
	pages = {22},
	file = {Kocsis et al. - Improved Monte-Carlo Search.pdf:/home/shadex91/Zotero/storage/RS2BMRXG/Kocsis et al. - Improved Monte-Carlo Search.pdf:application/pdf}
}

@article{silver_mastering_2017,
	title = {Mastering {Chess} and {Shogi} by {Self}-{Play} with a {General} {Reinforcement} {Learning} {Algorithm}},
	url = {http://arxiv.org/abs/1712.01815},
	abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
	urldate = {2020-07-30},
	journal = {arXiv:1712.01815 [cs]},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.01815},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/shadex91/Zotero/storage/Z43PHL2M/Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a Gene.pdf:application/pdf;arXiv.org Snapshot:/home/shadex91/Zotero/storage/8ZJHUH8J/1712.html:text/html}
}

@article{tian_elf_2019,
	title = {{ELF} {OpenGo}: {An} {Analysis} and {Open} {Reimplementation} of {AlphaZero}},
	shorttitle = {{ELF} {OpenGo}},
	url = {http://arxiv.org/abs/1902.04522},
	abstract = {The AlphaGo, AlphaGo Zero, and AlphaZero series of algorithms are remarkable demonstrations of deep reinforcement learning's capabilities, achieving superhuman performance in the complex game of Go with progressively increasing autonomy. However, many obstacles remain in the understanding of and usability of these promising approaches by the research community. Toward elucidating unresolved mysteries and facilitating future research, we propose ELF OpenGo, an open-source reimplementation of the AlphaZero algorithm. ELF OpenGo is the first open-source Go AI to convincingly demonstrate superhuman performance with a perfect (20:0) record against global top professionals. We apply ELF OpenGo to conduct extensive ablation studies, and to identify and analyze numerous interesting phenomena in both the model training and in the gameplay inference procedures. Our code, models, selfplay datasets, and auxiliary data are publicly available.},
	urldate = {2020-07-30},
	journal = {arXiv:1902.04522 [cs, stat]},
	author = {Tian, Yuandong and Ma, Jerry and Gong, Qucheng and Sengupta, Shubho and Chen, Zhuoyuan and Pinkerton, James and Zitnick, C. Lawrence},
	month = may,
	year = {2019},
	note = {arXiv: 1902.04522},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published as a conference paper at ICML 2019. This version contains supplementary appendices},
	file = {arXiv Fulltext PDF:/home/shadex91/Zotero/storage/5XELDB44/Tian et al. - 2019 - ELF OpenGo An Analysis and Open Reimplementation .pdf:application/pdf;arXiv.org Snapshot:/home/shadex91/Zotero/storage/V3UA8GRY/1902.html:text/html}
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2020-07-30},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report},
	file = {arXiv Fulltext PDF:/home/shadex91/Zotero/storage/DMQ3YB5Y/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/home/shadex91/Zotero/storage/5JP79NZS/1512.html:text/html}
}

@article{cazenave_ucd_2012,
	title = {{UCD} : {Upper} confidence bound for rooted directed acyclic graphs},
	volume = {34},
	shorttitle = {{UCD}},
	url = {https://hal.archives-ouvertes.fr/hal-01499672},
	doi = {10.1016/j.knosys.2011.11.014},
	abstract = {In this paper we present a framework for testing various algorithms that deal with transpositions in Monte-Carlo Tree Search (MCTS). We call this framework Upper Confidence bound for Direct acyclic graphs (UCD) as it constitutes an extension of Upper Confidence bound for Trees (UCT) for Direct acyclic graphs (DAG).When using transpositions in MCTS, a DAG is progressively developed instead of a tree. There are multiple ways to handle the exploration exploitation dilemma when dealing with transpositions. We propose parameterized ways to compute the mean of the child, the playouts of the parent and the playouts of the child. We test the resulting algorithms on several games. For all games, original configurations of our algorithms improve on state of the art algorithms.},
	urldate = {2020-07-30},
	journal = {Knowledge-Based Systems},
	author = {Cazenave, Tristan and Méhat, Jean and Saffidine, Abdallah},
	year = {2012},
	note = {Publisher: Elsevier},
	keywords = {Direct acyclic graph, Game tree search, Heuristic search, Monte-Carlo Tree Search, Transpositions, UCT Algorithm},
	pages = {26--33},
	file = {HAL PDF Full Text:/home/shadex91/Zotero/storage/PHULMR4N/Cazenave et al. - 2012 - UCD  Upper confidence bound for rooted directed a.pdf:application/pdf}
}

@misc{pons_vier_nodate,
	title = {Vier gewinnt {Löser}},
	url = {https://connect4.gamesolver.org/de/},
	abstract = {Spiele Vier Gewinnt online gegen einen perfekten Spieler},
	language = {de},
	urldate = {2020-07-30},
	journal = {Game Solver},
	author = {Pons, Pascal},
	note = {Library Catalog: connect4.gamesolver.org},
	file = {Snapshot:/home/shadex91/Zotero/storage/5542E6DC/connect4.gamesolver.org.html:text/html}
}

@misc{pons_solving_nodate,
	title = {Solving {Connect} 4: how to build a perfect {AI}},
	shorttitle = {Solving {Connect} 4},
	url = {http://blog.gamesolver.org/},
	language = {en},
	urldate = {2020-07-30},
	journal = {Solving Connect 4: how to build a perfect AI},
	author = {Pons, Pascal},
	note = {Library Catalog: blog.gamesolver.org},
	file = {Snapshot:/home/shadex91/Zotero/storage/HRW6F9IP/blog.gamesolver.org.html:text/html}
}

@misc{czarnogorski_monte_2018,
	title = {Monte {Carlo} {Tree} {Search} - beginners guide},
	url = {https://int8.io/monte-carlo-tree-search-beginners-guide/},
	abstract = {Monte Carlo Tree Search - the beginners guide with python code and references to monte carlo tree search application for Deepmind's AlphaGo},
	language = {en-GB},
	urldate = {2020-07-30},
	journal = {int8.io},
	author = {Czarnogorski, Kamil},
	month = mar,
	year = {2018},
	note = {Library Catalog: int8.io
Section: Monte Carlo Tree Search},
	file = {Snapshot:/home/shadex91/Zotero/storage/IWTBLPVN/monte-carlo-tree-search-beginners-guide.html:text/html}
}

@misc{baeldung_monte_2017,
	title = {Monte {Carlo} {Tree} {Search} for {Tic}-{Tac}-{Toe} {Game}},
	url = {https://www.baeldung.com/java-monte-carlo-tree-search},
	abstract = {Learn Monte Carlo Tree Search (MCTS) algorithm and its applications by exploring implementation for Tic-Tac-Toe game in Java.},
	language = {en-US},
	urldate = {2020-07-30},
	journal = {Baeldung},
	author = {{baeldung}},
	month = jun,
	year = {2017},
	note = {Library Catalog: www.baeldung.com},
	file = {Snapshot:/home/shadex91/Zotero/storage/67DEF6YY/java-monte-carlo-tree-search.html:text/html}
}

@misc{prasad_lessons_2018,
	title = {Lessons {From} {Implementing} {AlphaZero}},
	url = {https://medium.com/oracledevs/lessons-from-implementing-alphazero-7e36e9054191},
	abstract = {DeepMind’s AlphaZero publication was a landmark in reinforcement learning (RL) for board game play. The algorithm achieved superhuman…},
	language = {en},
	urldate = {2020-07-30},
	journal = {Medium},
	author = {Prasad, Aditya},
	month = jul,
	year = {2018},
	note = {Library Catalog: medium.com},
	file = {Snapshot:/home/shadex91/Zotero/storage/MN4XFX8M/lessons-from-implementing-alphazero-7e36e9054191.html:text/html}
}

@misc{bushaev_how_2018,
	title = {How do we ‘train’ neural networks ?},
	url = {https://towardsdatascience.com/how-do-we-train-neural-networks-edd985562b73},
	abstract = {I. Introduction},
	language = {en},
	urldate = {2020-07-30},
	journal = {Medium},
	author = {Bushaev, Vitaly},
	month = oct,
	year = {2018},
	note = {Library Catalog: towardsdatascience.com},
	file = {Snapshot:/home/shadex91/Zotero/storage/G2UWN5FY/how-do-we-train-neural-networks-edd985562b73.html:text/html}
}

@article{helmbold_all-moves-as-first_nodate,
	title = {All-{Moves}-{As}-{First} {Heuristics} in {Monte}-{Carlo} {Go}},
	abstract = {We present and explore the effectiveness of several variations on the All-Moves-As-First (AMAF) heuristic in Monte-Carlo Go. Our results show that: • Random play-outs provide more information about the goodness of moves made earlier in the play-out. • AMAF updates are not just a way to quickly initialize counts, they are useful after every play-out. • Updates even more aggressive than AMAF can be even more beneﬁcial.},
	language = {en},
	author = {Helmbold, David P and Parker-Wood, Aleatha},
	pages = {6},
	file = {Helmbold and Parker-Wood - All-Moves-As-First Heuristics in Monte-Carlo Go.pdf:/home/shadex91/Zotero/storage/XBWKK3C4/Helmbold and Parker-Wood - All-Moves-As-First Heuristics in Monte-Carlo Go.pdf:application/pdf}
}

@article{finnsson_simulation-based_nodate,
	title = {Simulation-{Based} {Approach} to {General} {Game} {Playing}},
	abstract = {The aim of General Game Playing (GGP) is to create intelligent agents that automatically learn how to play many different games at an expert level without any human intervention. The most successful GGP agents in the past have used traditional game-tree search combined with an automatically learned heuristic function for evaluating game states. In this paper we describe a GGP agent that instead uses a Monte Carlo/UCT simulation technique for action selection, an approach recently popularized in computer Go. Our GGP agent has proven its effectiveness by winning last year’s AAAI GGP Competition. Furthermore, we introduce and empirically evaluate a new scheme for automatically learning search-control knowledge for guiding the simulation playouts, showing that it offers signiﬁcant beneﬁts for a variety of games.},
	language = {en},
	author = {Finnsson, Hilmar and Björnsson, Yngvi},
	pages = {6},
	file = {Finnsson and Björnsson - Simulation-Based Approach to General Game Playing.pdf:/home/shadex91/Zotero/storage/ZDT3RERN/Finnsson and Björnsson - Simulation-Based Approach to General Game Playing.pdf:application/pdf}
}

@inproceedings{gelly_combining_2007,
	address = {Corvalis, Oregon, USA},
	series = {{ICML} '07},
	title = {Combining online and offline knowledge in {UCT}},
	isbn = {978-1-59593-793-3},
	url = {https://doi.org/10.1145/1273496.1273531},
	doi = {10.1145/1273496.1273531},
	abstract = {The UCT algorithm learns a value function online using sample-based search. The TD(λ) algorithm can learn a value function offline for the on-policy distribution. We consider three approaches for combining offline and online value functions in the UCT algorithm. First, the offline value function is used as a default policy during Monte-Carlo simulation. Second, the UCT value function is combined with a rapid online estimate of action values. Third, the offline value function is used as prior knowledge in the UCT search tree. We evaluate these algorithms in 9 x 9 Go against GnuGo 3.7.10. The first algorithm performs better than UCT with a random simulation policy, but surprisingly, worse than UCT with a weaker, handcrafted simulation policy. The second algorithm outperforms UCT altogether. The third algorithm outperforms UCT with handcrafted prior knowledge. We combine these algorithms in MoGo, the world's strongest 9 x 9 Go program. Each technique significantly improves MoGo's playing strength.},
	urldate = {2020-07-30},
	booktitle = {Proceedings of the 24th international conference on {Machine} learning},
	publisher = {Association for Computing Machinery},
	author = {Gelly, Sylvain and Silver, David},
	month = jun,
	year = {2007},
	pages = {273--280},
	file = {Submitted Version:/home/shadex91/Zotero/storage/PIPXRF8E/Gelly and Silver - 2007 - Combining online and offline knowledge in UCT.pdf:application/pdf}
}

@article{mehat_combining_2010,
	title = {Combining {UCT} and {Nested} {Monte} {Carlo} {Search} for {Single}-{Player} {General} {Game} {Playing}},
	volume = {2},
	issn = {1943-0698},
	doi = {10.1109/TCIAIG.2010.2088123},
	abstract = {Monte Carlo tree search (MCTS) has been recently very successful for game playing, particularly for games where the evaluation of a state is difficult to compute, such as Go or General Games. We compare nested Monte Carlo (NMC) search, upper confidence bounds for trees (UCT-T), UCT with transposition tables (UCT+T), and a simple combination of NMC and UCT+T (MAX) on single-player games of the past General Game Playing (GGP) competitions. We show that transposition tables improve UCT and that MAX is the best of these four algorithms. Using UCT+T, the program Ary won the 2009 GGP competition. MAX and NMC are slight improvements over this 2009 version.},
	number = {4},
	journal = {IEEE Transactions on Computational Intelligence and AI in Games},
	author = {Méhat, Jean and Cazenave, Tristan},
	month = dec,
	year = {2010},
	note = {Conference Name: IEEE Transactions on Computational Intelligence and AI in Games},
	keywords = {Algorithm design and analysis, Classification algorithms, computer games, Decision trees, Games, General Game Playing (GPP), MAX, Monte Carlo methods, nested Monte Carlo tree search, nested Monte-Carlo search, program Ary, single player general game playing, single-player games, transposition tables, tree searching, UCT+T, upper confidence bounds, upper confidence bounds for trees (UCT)},
	pages = {271--277},
	file = {IEEE Xplore Abstract Record:/home/shadex91/Zotero/storage/IAY4N6DM/5604665.html:text/html;Submitted Version:/home/shadex91/Zotero/storage/S5TGZS86/Méhat and Cazenave - 2010 - Combining UCT and Nested Monte Carlo Search for Si.pdf:application/pdf}
}

@inproceedings{vodopivec_enhancing_2014,
	address = {Dortmund, Germany},
	title = {Enhancing upper confidence bounds for trees with temporal difference values},
	isbn = {978-1-4799-3547-5},
	url = {http://ieeexplore.ieee.org/document/6932895/},
	doi = {10.1109/CIG.2014.6932895},
	abstract = {Upper conﬁdence bounds for trees (UCT) is one of the most popular and generally effective Monte Carlo tree search (MCTS) algorithms. However, in practice it is relatively weak when not aided by additional enhancements. Improving its performance without reducing generality is a current research challenge. We introduce a new domain-independent UCT enhancement based on the theory of reinforcement learning. Our approach estimates state values in the UCT tree by employing temporal difference (TD) learning, which is known to outperform plain Monte Carlo sampling in certain domains. We present three adaptations of the TD(������) algorithm to the UCT’s tree policy and backpropagation step. Evaluations on four games (Gomoku, Hex, Connect Four, and Tic Tac Toe) reveal that our approach increases UCT’s level of play comparably to the rapid action value estimation (RAVE) enhancement. Furthermore, it proves highly compatible with a modiﬁed all moves as ﬁrst heuristic, where it considerably outperforms RAVE. The ﬁndings suggest that integration of TD learning into MCTS deserves further research, which may form a new class of MCTS enhancements.},
	language = {en},
	urldate = {2020-07-30},
	booktitle = {2014 {IEEE} {Conference} on {Computational} {Intelligence} and {Games}},
	publisher = {IEEE},
	author = {Vodopivec, Tom and Ster, Branko},
	month = aug,
	year = {2014},
	pages = {1--8},
	file = {Vodopivec and Ster - 2014 - Enhancing upper confidence bounds for trees with t.pdf:/home/shadex91/Zotero/storage/VTG93JZD/Vodopivec and Ster - 2014 - Enhancing upper confidence bounds for trees with t.pdf:application/pdf}
}

@article{silver_mastering_2017-1,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature24270},
	doi = {10.1038/nature24270},
	abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.},
	language = {en},
	number = {7676},
	urldate = {2020-07-30},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	month = oct,
	year = {2017},
	note = {Number: 7676
Publisher: Nature Publishing Group},
	pages = {354--359},
	file = {Full Text PDF:/home/shadex91/Zotero/storage/GANLQK37/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:application/pdf;Snapshot:/home/shadex91/Zotero/storage/TQU767Y8/nature24270.html:text/html}
}

@article{gelly_monte-carlo_2011,
	title = {Monte-{Carlo} tree search and rapid action value estimation in computer {Go}},
	volume = {175},
	issn = {0004-3702},
	url = {http://www.sciencedirect.com/science/article/pii/S000437021100052X},
	doi = {10.1016/j.artint.2011.03.007},
	abstract = {A new paradigm for search, based on Monte-Carlo simulation, has revolutionised the performance of computer Go programs. In this article we describe two extensions to the Monte-Carlo tree search algorithm, which significantly improve the effectiveness of the basic algorithm. When we applied these two extensions to the Go program MoGo, it became the first program to achieve dan (master) level in 9×9 Go. In this article we survey the Monte-Carlo revolution in computer Go, outline the key ideas that led to the success of MoGo and subsequent Go programs, and provide for the first time a comprehensive description, in theory and in practice, of this extended framework for Monte-Carlo tree search.},
	language = {en},
	number = {11},
	urldate = {2020-07-30},
	journal = {Artificial Intelligence},
	author = {Gelly, Sylvain and Silver, David},
	month = jul,
	year = {2011},
	keywords = {Computer Go, Monte-Carlo, Reinforcement learning, Search},
	pages = {1856--1875},
	file = {ScienceDirect Snapshot:/home/shadex91/Zotero/storage/5V4JY7CK/S000437021100052X.html:text/html;ScienceDirect Full Text PDF:/home/shadex91/Zotero/storage/HPPL7SBR/Gelly and Silver - 2011 - Monte-Carlo tree search and rapid action value est.pdf:application/pdf}
}

@incollection{van_den_herik_score_2011,
	address = {Berlin, Heidelberg},
	title = {Score {Bounded} {Monte}-{Carlo} {Tree} {Search}},
	volume = {6515},
	isbn = {978-3-642-17927-3 978-3-642-17928-0},
	url = {http://link.springer.com/10.1007/978-3-642-17928-0_9},
	abstract = {Monte-Carlo Tree Search (MCTS) is a successful algorithm used in many state of the art game engines. We propose to improve a MCTS solver when a game has more than two outcomes. It is for example the case in games that can end in draw positions. In this case it improves signiﬁcantly a MCTS solver to take into account bounds on the possible scores of a node in order to select the nodes to explore. We apply our algorithm to solving Seki in the game of Go and to Connect Four.},
	language = {en},
	urldate = {2020-07-30},
	booktitle = {Computers and {Games}},
	publisher = {Springer Berlin Heidelberg},
	author = {Cazenave, Tristan and Saffidine, Abdallah},
	editor = {van den Herik, H. Jaap and Iida, Hiroyuki and Plaat, Aske},
	year = {2011},
	doi = {10.1007/978-3-642-17928-0_9},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {93--104},
	file = {Cazenave and Saffidine - 2011 - Score Bounded Monte-Carlo Tree Search.pdf:/home/shadex91/Zotero/storage/9YX8IQTQ/Cazenave and Saffidine - 2011 - Score Bounded Monte-Carlo Tree Search.pdf:application/pdf}
}

@article{browne_survey_2012,
	title = {A {Survey} of {Monte} {Carlo} {Tree} {Search} {Methods}},
	volume = {4},
	issn = {1943-0698},
	doi = {10.1109/TCIAIG.2012.2186810},
	abstract = {Monte Carlo tree search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the first five years of MCTS research. We outline the core algorithm's derivation, impart some structure on the many variations and enhancements that have been proposed, and summarize the results from the key game and nongame domains to which MCTS methods have been applied. A number of open research questions indicate that the field is ripe for future work.},
	number = {1},
	journal = {IEEE Transactions on Computational Intelligence and AI in Games},
	author = {Browne, Cameron B. and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M. and Cowling, Peter I. and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
	month = mar,
	year = {2012},
	note = {Conference Name: IEEE Transactions on Computational Intelligence and AI in Games},
	keywords = {Artificial intelligence, Artificial intelligence (AI), bandit-based methods, computer Go, Computers, Decision theory, game search, game theory, Game theory, Games, key game, Markov processes, MCTS research, Monte Carlo methods, Monte Carlo tree search (MCTS), Monte carlo tree search methods, nongame domains, random sampling generality, tree searching, upper confidence bounds (UCB), upper confidence bounds for trees (UCT)},
	pages = {1--43},
	file = {IEEE Xplore Full Text PDF:/home/shadex91/Zotero/storage/86I5KDW4/Browne et al. - 2012 - A Survey of Monte Carlo Tree Search Methods.pdf:application/pdf;IEEE Xplore Abstract Record:/home/shadex91/Zotero/storage/DMQGD9KW/6145622.html:text/html}
}

@inproceedings{childs_transpositions_2008,
	title = {Transpositions and move groups in {Monte} {Carlo} tree search},
	doi = {10.1109/CIG.2008.5035667},
	abstract = {Monte Carlo search, and specifically the UCT (Upper Confidence Bounds applied to Trees) algorithm, has contributed to a significant improvement in the game of Go and has received considerable attention in other applications. This article investigates two enhancements to the UCT algorithm. First, we consider the possible adjustments to UCT when the search tree is treated as a graph (and information amongst transpositions are shared). The second modification introduces move groupings, which may reduce the effective branching factor. Experiments with both enhancements were performed using artificial trees and in the game of Go. From the experimental results we conclude that both exploiting the graph structure and grouping moves may contribute to an increase in the playing strength of game programs using UCT.},
	booktitle = {2008 {IEEE} {Symposium} {On} {Computational} {Intelligence} and {Games}},
	author = {Childs, Benjamin E. and Brodeur, James H. and Kocsis, Levente},
	month = dec,
	year = {2008},
	note = {ISSN: 2325-4289},
	keywords = {Algorithm design and analysis, artificial trees, Automation, computer games, Computer science, effective branching factor, Electronic mail, game programs, graph structure, History, Monte Carlo methods, Monte Carlo tree search, Statistics, Tree data structures, Tree graphs, trees (mathematics), upper confidence bounds},
	pages = {389--395},
	file = {IEEE Xplore Full Text PDF:/home/shadex91/Zotero/storage/M8I3M8QP/Childs et al. - 2008 - Transpositions and move groups in Monte Carlo tree.pdf:application/pdf;IEEE Xplore Abstract Record:/home/shadex91/Zotero/storage/DWPLNBFS/5035667.html:text/html}
}

@book{geron_hands-machine_2019,
	edition = {2 edition},
	title = {Hands-{On} {Machine} {Learning} with {Scikit}-{Learn}, {Keras}, and {TensorFlow}: {Concepts}, {Tools}, and {Techniques} to {Build} {Intelligent} {Systems}},
	isbn = {978-1-4920-3264-9},
	shorttitle = {Hands-{On} {Machine} {Learning} with {Scikit}-{Learn}, {Keras}, and {TensorFlow}},
	abstract = {Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how.By using concrete examples, minimal theory, and two production-ready Python frameworks—Scikit-Learn and TensorFlow—author Aurélien Géron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You’ll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what you’ve learned, all you need is programming experience to get started.Explore the machine learning landscape, particularly neural netsUse Scikit-Learn to track an example machine-learning project end-to-endExplore several training models, including support vector machines, decision trees, random forests, and ensemble methodsUse the TensorFlow library to build and train neural netsDive into neural net architectures, including convolutional nets, recurrent nets, and deep reinforcement learningLearn techniques for training and scaling deep neural nets},
	language = {English},
	publisher = {O'Reilly Media},
	author = {Géron, Aurélien},
	month = oct,
	year = {2019}
}

@book{rashid_neuronale_2017,
	address = {Heidelberg},
	title = {Neuronale {Netze} selbst programmieren: {Ein} verständlicher {Einstieg} mit {Python}},
	isbn = {978-3-96009-043-4},
	shorttitle = {Neuronale {Netze} selbst programmieren},
	abstract = {Neuronale Netze sind Schlüsselelemente des Deep Learning und der Künstlichen Intelligenz, die heute zu Erstaunlichem in der Lage sind. Sie sind Grundlage vieler Anwendungen im Alltag wie beispielsweise Spracherkennung, Gesichtserkennung auf Fotos oder die Umwandlung von Sprache in Text. Dennoch verstehen nur wenige, wie neuronale Netze tatsächlich funktionieren.  Dieses Buch nimmt Sie mit auf eine unterhaltsame Reise, die mit ganz einfachen Ideen beginnt und Ihnen Schritt für Schritt zeigt, wie neuronale Netze arbeiten:  - Zunächst lernen Sie die mathematischen Konzepte kennen, die den neuronalen Netzen zugrunde liegen. Dafür brauchen Sie keine tieferen Mathematikkenntnisse, denn alle mathematischen Ideen werden behutsam und mit vielen Illustrationen und Beispielen erläutert. Eine Kurzeinführung in die Analysis unterstützt Sie dabei.  - Dann geht es in die Praxis: Nach einer Einführung in die populäre und leicht zu lernende Programmiersprache Python bauen Sie allmählich Ihr eigenes neuronales Netz mit Python auf. Sie bringen ihm bei, handgeschriebene Zahlen zu erkennen, bis es eine Performance wie ein professionell entwickeltes Netz erreicht.  - Im nächsten Schritt tunen Sie die Leistung Ihres neuronalen Netzes so weit, dass es eine Zahlenerkennung von 98 \% erreicht – nur mit einfachen Ideen und simplem Code. Sie testen das Netz mit Ihrer eigenen Handschrift und werfen noch einen Blick in das mysteriöse Innere eines neuronalen Netzes.  - Zum Schluss lassen Sie das neuronale Netz auf einem Raspberry Pi Zero laufen.  Tariq Rashid erklärt diese schwierige Materie außergewöhnlich klar und verständlich, dadurch werden neuronale Netze für jeden Interessierten zugänglich und praktisch nachvollziehbar.},
	language = {Deutsch},
	publisher = {O'Reilly},
	author = {Rashid, Tariq},
	translator = {Langenau, Frank},
	month = apr,
	year = {2017}
}

@book{chollet_deep_2017,
	address = {Shelter Island, New York},
	edition = {1st},
	title = {Deep {Learning} with {Python}},
	isbn = {978-1-61729-443-3},
	abstract = {SummaryDeep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google AI researcher François Chollet, this book builds your understanding through intuitive explanations and practical examples.Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications.About the TechnologyMachine learning has made remarkable progress in recent years. We went from near-unusable speech and image recognition, to near-human accuracy. We went from machines that couldn't beat a serious Go player, to defeating a world champion. Behind this progress is deep learning—a combination of engineering advances, best practices, and theory that enables a wealth of previously impossible smart applications.About the BookDeep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google AI researcher François Chollet, this book builds your understanding through intuitive explanations and practical examples. You'll explore challenging concepts and practice with applications in computer vision, natural-language processing, and generative models. By the time you finish, you'll have the knowledge and hands-on skills to apply deep learning in your own projects. What's InsideDeep learning from first principlesSetting up your own deep-learning environment Image-classification modelsDeep learning for text and sequencesNeural style transfer, text generation, and image generationAbout the ReaderReaders need intermediate Python skills. No previous experience with Keras, TensorFlow, or machine learning is required.About the AuthorFrançois Chollet works on deep learning at Google in Mountain View, CA. He is the creator of the Keras deep-learning library, as well as a contributor to the TensorFlow machine-learning framework. He also does deep-learning research, with a focus on computer vision and the application of machine learning to formal reasoning. His papers have been published at major conferences in the field, including the Conference on Computer Vision and Pattern Recognition (CVPR), the Conference and Workshop on Neural Information Processing Systems (NIPS), the International Conference on Learning Representations (ICLR), and others.Table of ContentsPART 1 - FUNDAMENTALS OF DEEP LEARNING What is deep learning?Before we begin: the mathematical building blocks of neural networks Getting started with neural networksFundamentals of machine learningPART 2 - DEEP LEARNING IN PRACTICEDeep learning for computer visionDeep learning for text and sequencesAdvanced deep-learning best practicesGenerative deep learningConclusionsappendix A - Installing Keras and its dependencies on Ubuntuappendix B - Running Jupyter notebooks on an EC2 GPU instance},
	language = {Englisch},
	publisher = {Manning Publications},
	author = {Chollet, François},
	year = {2017}
}

@misc{ronaghan_deep_2019,
	title = {Deep {Learning}: {Which} {Loss} and {Activation} {Functions} should {I} use?},
	shorttitle = {Deep {Learning}},
	url = {https://towardsdatascience.com/deep-learning-which-loss-and-activation-functions-should-i-use-ac02f1c56aa8},
	abstract = {The purpose of this post is to provide guidance on which combination of final-layer activation function and loss function should be used in…},
	language = {en},
	urldate = {2020-07-30},
	journal = {Medium},
	author = {Ronaghan, Stacey},
	month = aug,
	year = {2019},
	note = {Library Catalog: towardsdatascience.com},
	file = {Snapshot:/home/shadex91/Zotero/storage/3MPNN7JV/deep-learning-which-loss-and-activation-functions-should-i-use-ac02f1c56aa8.html:text/html}
}

@misc{mao_unveil_nodate,
	title = {Unveil {AlphaZero}},
	url = {https://leimao.github.io/article/Alpha-Zero/},
	abstract = {Hello Underworld.},
	language = {en},
	urldate = {2020-07-30},
	journal = {Lei Mao's Log Book},
	author = {Mao, Lei},
	note = {Library Catalog: leimao.github.io},
	file = {Snapshot:/home/shadex91/Zotero/storage/JSXRB4IA/Alpha-Zero.html:text/html}
}

@misc{noauthor_7_nodate,
	title = {7 {Types} of {Activation} {Functions} in {Neural} {Networks}: {How} to {Choose}?},
	shorttitle = {7 {Types} of {Activation} {Functions} in {Neural} {Networks}},
	url = {https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/},
	abstract = {Understand the evolution of different types of activation functions in neural network and learn the pros and cons of linear, step, ReLU, PRLeLU, Softmax and Swish.},
	language = {en-US},
	urldate = {2020-07-30},
	journal = {MissingLink.ai},
	note = {Library Catalog: missinglink.ai},
	file = {Snapshot:/home/shadex91/Zotero/storage/K596Q2F9/7-types-neural-network-activation-functions-right.html:text/html}
}

@misc{noauthor_complete_nodate,
	title = {Complete {Guide} to {Artificial} {Neural} {Network} {Concepts} \& {Models}},
	url = {https://missinglink.ai/guides/neural-network-concepts/complete-guide-artificial-neural-networks/},
	abstract = {Learn fundamental concepts of neural networks - backpropagation, activation functions, hyperparameters, RNN, CNN and more.},
	language = {en-US},
	urldate = {2020-07-30},
	journal = {MissingLink.ai},
	note = {Library Catalog: missinglink.ai},
	file = {Snapshot:/home/shadex91/Zotero/storage/5GP44PLE/complete-guide-artificial-neural-networks.html:text/html}
}
