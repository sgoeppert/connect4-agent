
@misc{allenExpertPlayConnectFour,
  title = {{Expert Play in Connect-Four}},
  author = {Allen, James D.},
  file = {/home/shadex91/Zotero/storage/I8S452VB/c4.html},
  howpublished = {http://tromp.github.io/c4.html},
  language = {Englisch}
}

@phdthesis{allisKnowledgeBasedApproachConnectFour1988,
  title = {{A Knowledge-Based Approach of Connect-Four: The Game Is Solved: White Wins}},
  shorttitle = {{A Knowledge-Based Approach of Connect-Four}},
  author = {Allis, Victor},
  year = {1988},
  month = dec,
  address = {{Amsterdam}},
  abstract = {A Shannon C-type strategy program, VICTOR, is written for Connect-Four, based on nine strategic rules. Each of these rules is proven to be correct, implying that conclusions made by VICTOR are correct.},
  file = {/home/shadex91/Zotero/storage/ACF84643/Allis - 1988 - A Knowledge-Based Approach of Connect-Four The Ga.pdf},
  language = {Englisch},
  school = {Vrije Universiteit}
}

@misc{AlphaGoStoryFar,
  title = {{AlphaGo: The story so far}},
  shorttitle = {{AlphaGo}},
  abstract = {AlphaGo is the first computer program to defeat a professional human Go player, a landmark achievement that experts believe was a decade ahead of its time.},
  file = {/home/shadex91/Zotero/storage/YUUWN982/alphago-the-story-so-far.html},
  howpublished = {https://deepmind.com/research/case-studies/alphago-the-story-so-far},
  journal = {Deepmind},
  language = {Englisch}
}

@misc{AlphaGoStoryFara,
  title = {{AlphaGo: The story so far}},
  shorttitle = {{AlphaGo}},
  abstract = {AlphaGo is the first computer program to defeat a professional human Go player, a landmark achievement that experts believe was a decade ahead of its time.},
  file = {/home/shadex91/Zotero/storage/I9UBKFL6/alphago-the-story-so-far.html},
  howpublished = {/research/case-studies/alphago-the-story-so-far},
  journal = {Deepmind},
  language = {ALL}
}

@article{auerFinitetimeAnalysisMultiarmed2002,
  title = {Finite-Time {{Analysis}} of the {{Multiarmed Bandit Problem}}},
  author = {Auer, Peter and {Cesa-Bianchi}, Nicol{\`o} and Fischer, Paul},
  year = {2002},
  month = may,
  volume = {47},
  pages = {235--256},
  issn = {1573-0565},
  doi = {10.1023/A:1013689704352},
  abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
  file = {/home/shadex91/Zotero/storage/BZSJZ7Y5/Auer et al. - 2002 - Finite-time Analysis of the Multiarmed Bandit Prob.pdf},
  journal = {Machine Learning},
  language = {en},
  number = {2}
}

@article{browneSurveyMonteCarlo2012,
  title = {A {{Survey}} of {{Monte Carlo Tree Search Methods}}},
  author = {Browne, Cameron B. and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M. and Cowling, Peter I. and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
  year = {2012},
  month = mar,
  volume = {4},
  pages = {1--43},
  issn = {1943-0698},
  doi = {10.1109/TCIAIG.2012.2186810},
  abstract = {Monte Carlo tree search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the first five years of MCTS research. We outline the core algorithm's derivation, impart some structure on the many variations and enhancements that have been proposed, and summarize the results from the key game and nongame domains to which MCTS methods have been applied. A number of open research questions indicate that the field is ripe for future work.},
  file = {/home/shadex91/Zotero/storage/86I5KDW4/Browne et al. - 2012 - A Survey of Monte Carlo Tree Search Methods.pdf;/home/shadex91/Zotero/storage/DMQGD9KW/6145622.html},
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  keywords = {Artificial intelligence,Artificial intelligence (AI),bandit-based methods,computer Go,Computers,Decision theory,game search,game theory,Game theory,Games,key game,Markov processes,MCTS research,Monte Carlo methods,Monte Carlo tree search (MCTS),Monte carlo tree search methods,nongame domains,random sampling generality,tree searching,upper confidence bounds (UCB),upper confidence bounds for trees (UCT)},
  number = {1}
}

@misc{burnmeisterCSTR339ComputerGo,
  title = {{{CS}}-{{TR}}-339 {{Computer Go Tech Report}}},
  author = {Burnmeister, Jay and Wiles, Janet},
  file = {/home/shadex91/Zotero/storage/I6KNRIDD/CS-TR-339.html},
  howpublished = {http://staff.itee.uq.edu.au/janetw/Computer\%20Go/CS-TR-339.html\#6.2}
}

@misc{bushaevHowWeTrain2018,
  title = {How Do We `Train' Neural Networks ?},
  author = {Bushaev, Vitaly},
  year = {2018},
  month = oct,
  abstract = {I. Introduction},
  file = {/home/shadex91/Zotero/storage/G2UWN5FY/how-do-we-train-neural-networks-edd985562b73.html},
  journal = {Medium},
  language = {en}
}

@misc{cazenaveExampleCutScore,
  title = {Example of a Cut in Score Bounded {{MCTS}}},
  author = {Cazenave, Tristan and Saffidine, Abdallah}
}

@incollection{cazenaveScoreBoundedMonteCarlo2011,
  title = {{Score Bounded Monte-Carlo Tree Search}},
  booktitle = {{Computers and Games}},
  author = {Cazenave, Tristan and Saffidine, Abdallah},
  editor = {{van den Herik}, H. Jaap and Iida, Hiroyuki and Plaat, Aske},
  year = {2011},
  volume = {6515},
  pages = {93--104},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_9},
  abstract = {Monte-Carlo Tree Search (MCTS) is a successful algorithm used in many state of the art game engines. We propose to improve a MCTS solver when a game has more than two outcomes. It is for example the case in games that can end in draw positions. In this case it improves significantly a MCTS solver to take into account bounds on the possible scores of a node in order to select the nodes to explore. We apply our algorithm to solving Seki in the game of Go and to Connect Four.},
  file = {/home/shadex91/Zotero/storage/9YX8IQTQ/Cazenave and Saffidine - 2011 - Score Bounded Monte-Carlo Tree Search.pdf},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  language = {Englisch}
}

@article{cazenaveUCDUpperConfidence2012,
  title = {{{UCD}} : {{Upper}} Confidence Bound for Rooted Directed Acyclic Graphs},
  shorttitle = {{{UCD}}},
  author = {Cazenave, Tristan and M{\'e}hat, Jean and Saffidine, Abdallah},
  year = {2012},
  volume = {34},
  pages = {26--33},
  publisher = {{Elsevier}},
  doi = {10.1016/j.knosys.2011.11.014},
  abstract = {In this paper we present a framework for testing various algorithms that deal with transpositions in Monte-Carlo Tree Search (MCTS). We call this framework Upper Confidence bound for Direct acyclic graphs (UCD) as it constitutes an extension of Upper Confidence bound for Trees (UCT) for Direct acyclic graphs (DAG).When using transpositions in MCTS, a DAG is progressively developed instead of a tree. There are multiple ways to handle the exploration exploitation dilemma when dealing with transpositions. We propose parameterized ways to compute the mean of the child, the playouts of the parent and the playouts of the child. We test the resulting algorithms on several games. For all games, original configurations of our algorithms improve on state of the art algorithms.},
  file = {/home/shadex91/Zotero/storage/PHULMR4N/Cazenave et al. - 2012 - UCD  Upper confidence bound for rooted directed a.pdf},
  journal = {Knowledge-Based Systems},
  keywords = {Direct acyclic graph,Game tree search,Heuristic search,Monte-Carlo Tree Search,Transpositions,UCT Algorithm}
}

@inproceedings{childsTranspositionsMoveGroups2008,
  title = {Transpositions and Move Groups in {{Monte Carlo}} Tree Search},
  booktitle = {2008 {{IEEE Symposium On Computational Intelligence}} and {{Games}}},
  author = {Childs, Benjamin E. and Brodeur, James H. and Kocsis, Levente},
  year = {2008},
  month = dec,
  pages = {389--395},
  issn = {2325-4289},
  doi = {10.1109/CIG.2008.5035667},
  abstract = {Monte Carlo search, and specifically the UCT (Upper Confidence Bounds applied to Trees) algorithm, has contributed to a significant improvement in the game of Go and has received considerable attention in other applications. This article investigates two enhancements to the UCT algorithm. First, we consider the possible adjustments to UCT when the search tree is treated as a graph (and information amongst transpositions are shared). The second modification introduces move groupings, which may reduce the effective branching factor. Experiments with both enhancements were performed using artificial trees and in the game of Go. From the experimental results we conclude that both exploiting the graph structure and grouping moves may contribute to an increase in the playing strength of game programs using UCT.},
  file = {/home/shadex91/Zotero/storage/M8I3M8QP/Childs et al. - 2008 - Transpositions and move groups in Monte Carlo tree.pdf;/home/shadex91/Zotero/storage/DWPLNBFS/5035667.html},
  keywords = {Algorithm design and analysis,artificial trees,Automation,computer games,Computer science,effective branching factor,Electronic mail,game programs,graph structure,History,Monte Carlo methods,Monte Carlo tree search,Statistics,Tree data structures,Tree graphs,trees (mathematics),upper confidence bounds}
}

@book{cholletDeepLearningPython2017,
  title = {{Deep Learning with Python}},
  author = {Chollet, Fran{\c c}ois},
  year = {2017},
  month = dec,
  edition = {1st},
  publisher = {{Manning Publications}},
  address = {{Shelter Island, New York}},
  abstract = {SummaryDeep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google AI researcher Fran\c{c}ois Chollet, this book builds your understanding through intuitive explanations and practical examples.Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications.About the TechnologyMachine learning has made remarkable progress in recent years. We went from near-unusable speech and image recognition, to near-human accuracy. We went from machines that couldn't beat a serious Go player, to defeating a world champion. Behind this progress is deep learning\textemdash a combination of engineering advances, best practices, and theory that enables a wealth of previously impossible smart applications.About the BookDeep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google AI researcher Fran\c{c}ois Chollet, this book builds your understanding through intuitive explanations and practical examples. You'll explore challenging concepts and practice with applications in computer vision, natural-language processing, and generative models. By the time you finish, you'll have the knowledge and hands-on skills to apply deep learning in your own projects. What's InsideDeep learning from first principlesSetting up your own deep-learning environment Image-classification modelsDeep learning for text and sequencesNeural style transfer, text generation, and image generationAbout the ReaderReaders need intermediate Python skills. No previous experience with Keras, TensorFlow, or machine learning is required.About the AuthorFran\c{c}ois Chollet works on deep learning at Google in Mountain View, CA. He is the creator of the Keras deep-learning library, as well as a contributor to the TensorFlow machine-learning framework. He also does deep-learning research, with a focus on computer vision and the application of machine learning to formal reasoning. His papers have been published at major conferences in the field, including the Conference on Computer Vision and Pattern Recognition (CVPR), the Conference and Workshop on Neural Information Processing Systems (NIPS), the International Conference on Learning Representations (ICLR), and others.Table of ContentsPART 1 - FUNDAMENTALS OF DEEP LEARNING What is deep learning?Before we begin: the mathematical building blocks of neural networks Getting started with neural networksFundamentals of machine learningPART 2 - DEEP LEARNING IN PRACTICEDeep learning for computer visionDeep learning for text and sequencesAdvanced deep-learning best practicesGenerative deep learningConclusionsappendix A - Installing Keras and its dependencies on Ubuntuappendix B - Running Jupyter notebooks on an EC2 GPU instance},
  isbn = {978-1-61729-443-3},
  language = {Englisch}
}

@incollection{coulomEfficientSelectivityBackup2007,
  title = {{Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search}},
  booktitle = {{Computers and Games}},
  author = {Coulom, R{\'e}mi},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and {van den Herik}, H. Jaap and Ciancarini, Paolo and Donkers, H. H. L. M.},
  year = {2007},
  volume = {4630},
  pages = {72--83},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_7},
  abstract = {Monte-Carlo evaluation consists in estimating a position by averaging the outcome of several random continuations, and can serve as an evaluation function at the leaves of a min-max tree. This paper presents a new framework to combine tree search with Monte-Carlo evaluation, that does not separate between a min-max phase and a MonteCarlo phase. Instead of backing-up the min-max value close to the root, and the average value at some depth, a more general backup operator is defined that progressively changes from averaging to min-max as the number of simulations grows. This approach provides a fine-grained control of the tree growth, at the level of individual simulations, and allows efficient selectivity methods. This algorithm was implemented in a 9 \texttimes{} 9 Go-playing program, Crazy Stone, that won the 10th KGS computer-Go tournament.},
  file = {/home/shadex91/Zotero/storage/RXCB9L97/Coulom - 2007 - Efficient Selectivity and Backup Operators in Mont.pdf},
  isbn = {978-3-540-75537-1 978-3-540-75538-8},
  language = {Englisch}
}

@misc{czarnogorskiMonteCarloTree2018,
  title = {Monte {{Carlo Tree Search}} - Beginners Guide},
  author = {Czarnogorski, Kamil},
  year = {2018},
  month = mar,
  abstract = {Monte Carlo Tree Search - the beginners guide with python code and references to monte carlo tree search application for Deepmind's AlphaGo},
  chapter = {Monte Carlo Tree Search},
  file = {/home/shadex91/Zotero/storage/IWTBLPVN/monte-carlo-tree-search-beginners-guide.html},
  journal = {int8.io},
  language = {en-GB}
}

@misc{en:user:gdrFirstTwoPly2007,
  title = {First Two Ply of a Game Tree for Tic-Tac-Toe},
  author = {{en:User:Gdr}, original by, Traced by User:Stannered},
  year = {2007},
  month = apr,
  copyright = {Permission is granted to copy, distribute and/or modify this document under the terms of the GNU Free Documentation License, Version 1.2 or any later version published by the Free Software Foundation; with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A copy of the license is included in the section entitled GNU Free Documentation License.http://www.gnu.org/copyleft/fdl.htmlGFDLGNU Free Documentation Licensetruetrue},
  file = {/home/shadex91/Zotero/storage/69Z3JJJZ/index.html}
}

@misc{en:user:gdrFirstTwoPly2007a,
  title = {First Two Ply of a Game Tree for Tic-Tac-Toe},
  author = {{en:User:Gdr}, original by, Traced by User:Stannered},
  year = {2007},
  month = apr,
  copyright = {Permission is granted to copy, distribute and/or modify this document under the terms of the GNU Free Documentation License, Version 1.2 or any later version published by the Free Software Foundation; with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A copy of the license is included in the section entitled GNU Free Documentation License.http://www.gnu.org/copyleft/fdl.htmlGFDLGNU Free Documentation Licensetruetrue},
  file = {/home/shadex91/Zotero/storage/4PQCZI5I/index.html}
}

@inproceedings{finnssonSimulationbasedApproachGeneral2008,
  title = {Simulation-Based Approach to General Game Playing},
  booktitle = {Proceedings of the 23rd National Conference on {{Artificial}} Intelligence - {{Volume}} 1},
  author = {Finnsson, Hilmar and Bj{\"o}rnsson, Yngvi},
  year = {2008},
  month = jul,
  pages = {259--264},
  publisher = {{AAAI Press}},
  address = {{Chicago, Illinois}},
  abstract = {The aim of General Game Playing (GGP) is to create intelligent agents that automatically learn how to play many different games at an expert level without any human intervention. The most successful GGP agents in the past have used traditional game-tree search combined with an automatically learned heuristic function for evaluating game states. In this paper we describe a GGP agent that instead uses a Monte Carlo/UCT simulation technique for action selection, an approach recently popularized in computer Go. Our GGP agent has proven its effectiveness by winning last year s AAAI GGP Competition. Furthermore, we introduce and empirically evaluate a new scheme for automatically learning search-control knowledge for guiding the simulation playouts, showing that it offers significant benefits for a variety of games.},
  file = {/home/shadex91/Zotero/storage/ZDT3RERN/Finnsson and Björnsson - Simulation-Based Approach to General Game Playing.pdf},
  isbn = {978-1-57735-368-3},
  series = {{{AAAI}}'08}
}

@inproceedings{gellyCombiningOnlineOffline2007,
  title = {Combining Online and Offline Knowledge in {{UCT}}},
  booktitle = {Proceedings of the 24th International Conference on {{Machine}} Learning},
  author = {Gelly, Sylvain and Silver, David},
  year = {2007},
  month = jun,
  pages = {273--280},
  publisher = {{Association for Computing Machinery}},
  address = {{Corvalis, Oregon, USA}},
  doi = {10.1145/1273496.1273531},
  abstract = {The UCT algorithm learns a value function online using sample-based search. The TD({$\lambda$}) algorithm can learn a value function offline for the on-policy distribution. We consider three approaches for combining offline and online value functions in the UCT algorithm. First, the offline value function is used as a default policy during Monte-Carlo simulation. Second, the UCT value function is combined with a rapid online estimate of action values. Third, the offline value function is used as prior knowledge in the UCT search tree. We evaluate these algorithms in 9 x 9 Go against GnuGo 3.7.10. The first algorithm performs better than UCT with a random simulation policy, but surprisingly, worse than UCT with a weaker, handcrafted simulation policy. The second algorithm outperforms UCT altogether. The third algorithm outperforms UCT with handcrafted prior knowledge. We combine these algorithms in MoGo, the world's strongest 9 x 9 Go program. Each technique significantly improves MoGo's playing strength.},
  file = {/home/shadex91/Zotero/storage/PIPXRF8E/Gelly and Silver - 2007 - Combining online and offline knowledge in UCT.pdf},
  isbn = {978-1-59593-793-3},
  series = {{{ICML}} '07}
}

@article{gellyMonteCarloTreeSearch2011,
  title = {Monte-{{Carlo}} Tree Search and Rapid Action Value Estimation in Computer {{Go}}},
  author = {Gelly, Sylvain and Silver, David},
  year = {2011},
  month = jul,
  volume = {175},
  pages = {1856--1875},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2011.03.007},
  abstract = {A new paradigm for search, based on Monte-Carlo simulation, has revolutionised the performance of computer Go programs. In this article we describe two extensions to the Monte-Carlo tree search algorithm, which significantly improve the effectiveness of the basic algorithm. When we applied these two extensions to the Go program MoGo, it became the first program to achieve dan (master) level in 9\texttimes 9 Go. In this article we survey the Monte-Carlo revolution in computer Go, outline the key ideas that led to the success of MoGo and subsequent Go programs, and provide for the first time a comprehensive description, in theory and in practice, of this extended framework for Monte-Carlo tree search.},
  file = {/home/shadex91/Zotero/storage/HPPL7SBR/Gelly and Silver - 2011 - Monte-Carlo tree search and rapid action value est.pdf;/home/shadex91/Zotero/storage/5V4JY7CK/S000437021100052X.html},
  journal = {Artificial Intelligence},
  keywords = {Computer Go,Monte-Carlo,Reinforcement learning,Search},
  language = {en},
  number = {11}
}

@article{GeloesteSpiele2019,
  title = {{Gel\"oste Spiele}},
  year = {2019},
  month = mar,
  abstract = {Ein zufallsfreies Zwei-Personen-Spiel mit perfekter Information kann in unterschiedlicher Weise gel\"ost werden:

Sehr schwach gel\"ost (engl. ultra weakly solved) ist ein Spiel, wenn man f\"ur die Startposition des Spieles dasjenige Spielergebnis bestimmen kann, das jeder der beiden Spieler unabh\"angig von der Spielweise seines Gegners mindestens erzwingen kann. Ein diesbez\"uglicher Nachweis muss \"uber die daf\"ur notwendigen Spielweisen keine Aussage machen.
Schwach gel\"ost ist ein Spiel, wenn dar\"uber hinaus ein praktisch realisierbarer Algorithmus angegeben werden kann, mit dem die beidseitig optimalen Spielweisen ausgehend von der Startposition des Spiels bestimmt werden k\"onnen.
Stark gel\"ost ist ein Spiel, wenn ein allgemeiner, praktisch realisierbarer Algorithmus existiert, mit dem f\"ur jede Position ein optimaler Zug berechnet werden kann. Im Unterschied zu schwach gel\"osten Spielen muss dieser Algorithmus auch f\"ur solche Positionen funktionieren, die ausgehend von der Ausgangsposition nur bei fehlerhafter Spielweise vorkommen.Wichtig ist die Anforderung eines praktisch (auf einem Computer) realisierbaren Algorithmus, da mit dem Minimax-Algorithmus stets ein allgemeines Verfahren existiert, mit dem theoretisch f\"ur jede Position eines endlichen Zwei-Personen-Spiels mit vollst\"andiger Information ein optimaler Zug berechnet werden kann.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {/home/shadex91/Zotero/storage/39KRVIKE/index.html},
  journal = {Wikipedia},
  language = {de}
}

@book{geronHandsOnMachineLearning2019,
  title = {Hands-{{On Machine Learning}} with {{Scikit}}-{{Learn}}, {{Keras}}, and {{TensorFlow}}: {{Concepts}}, {{Tools}}, and {{Techniques}} to {{Build Intelligent Systems}}},
  shorttitle = {Hands-{{On Machine Learning}} with {{Scikit}}-{{Learn}}, {{Keras}}, and {{TensorFlow}}},
  author = {G{\'e}ron, Aur{\'e}lien},
  year = {2019},
  month = oct,
  edition = {2 edition},
  publisher = {{O'Reilly Media}},
  abstract = {Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how.By using concrete examples, minimal theory, and two production-ready Python frameworks\textemdash Scikit-Learn and TensorFlow\textemdash author Aur\'elien G\'eron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what you've learned, all you need is programming experience to get started.Explore the machine learning landscape, particularly neural netsUse Scikit-Learn to track an example machine-learning project end-to-endExplore several training models, including support vector machines, decision trees, random forests, and ensemble methodsUse the TensorFlow library to build and train neural netsDive into neural net architectures, including convolutional nets, recurrent nets, and deep reinforcement learningLearn techniques for training and scaling deep neural nets},
  isbn = {978-1-4920-3264-9},
  language = {English}
}

@article{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archivePrefix = {arXiv},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  file = {/home/shadex91/Zotero/storage/DMQ3YB5Y/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;/home/shadex91/Zotero/storage/5JP79NZS/1512.html},
  journal = {arXiv:1512.03385 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{helmboldAllMovesAsFirstHeuristicsMonteCarlo,
  title = {All-{{Moves}}-{{As}}-{{First Heuristics}} in {{Monte}}-{{Carlo Go}}},
  author = {Helmbold, David P and {Parker-Wood}, Aleatha},
  pages = {6},
  abstract = {We present and explore the effectiveness of several variations on the All-Moves-As-First (AMAF) heuristic in Monte-Carlo Go. Our results show that: \textbullet{} Random play-outs provide more information about the goodness of moves made earlier in the play-out. \textbullet{} AMAF updates are not just a way to quickly initialize counts, they are useful after every play-out. \textbullet{} Updates even more aggressive than AMAF can be even more beneficial.},
  file = {/home/shadex91/Zotero/storage/XBWKK3C4/Helmbold and Parker-Wood - All-Moves-As-First Heuristics in Monte-Carlo Go.pdf},
  language = {en}
}

@misc{IBM100DeepBlue2012,
  title = {{{IBM100}} - {{Deep Blue}}},
  year = {2012},
  month = mar,
  publisher = {{IBM Corporation}},
  copyright = {\textcopyright{} Copyright IBM Corp. 2011},
  file = {/home/shadex91/Zotero/storage/ZTVDBNUQ/deepblue.html},
  howpublished = {http://www-03.ibm.com/ibm/history/ibm100/us/en/icons/deepblue/},
  language = {en-US},
  type = {{{CTB14}}}
}

@inproceedings{jinAutoKerasEfficientNeural2019,
  title = {Auto-{{Keras}}: {{An Efficient Neural Architecture Search System}}},
  shorttitle = {Auto-{{Keras}}},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Jin, Haifeng and Song, Qingquan and Hu, Xia},
  year = {2019},
  month = jul,
  pages = {1946--1956},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3292500.3330648},
  abstract = {Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet, PNAS, usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search. The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space. Extensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras. The code and documentation are available at https://autokeras.com. The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits.},
  file = {/home/shadex91/Zotero/storage/LUW95ZLB/Jin et al. - 2019 - Auto-Keras An Efficient Neural Architecture Searc.pdf},
  isbn = {978-1-4503-6201-6},
  keywords = {automated machine learning,automl,bayesian optimization,network morphis,neural architecture search},
  series = {{{KDD}} '19}
}

@inproceedings{kocsisBanditBasedMonteCarlo2006,
  title = {Bandit {{Based Monte}}-{{Carlo Planning}}},
  booktitle = {Machine {{Learning}}: {{ECML}} 2006},
  author = {Kocsis, Levente and Szepesv{\'a}ri, Csaba},
  editor = {F{\"u}rnkranz, Johannes and Scheffer, Tobias and Spiliopoulou, Myra},
  year = {2006},
  pages = {282--293},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11871842_29},
  abstract = {For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.},
  file = {/home/shadex91/Zotero/storage/CDBVK6CT/Kocsis and Szepesvári - 2006 - Bandit Based Monte-Carlo Planning.pdf},
  isbn = {978-3-540-46056-5},
  keywords = {Bandit Problem,Drift Condition,Failure Probability,Multiarmed Bandit,Multiarmed Bandit Problem},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{kocsisImprovedMonteCarloSearch,
  title = {Improved {{Monte}}-{{Carlo Search}}},
  author = {Kocsis, Levente and Szepesvari, Csaba and Willemson, Jan},
  pages = {22},
  abstract = {Monte-Carlo search has been successful in many non-deterministic games, and recently in deterministic games with high branching factor. One of the drawbacks of the current approaches is that even if the iterative process would last for a very long time, the selected move does not necessarily converge to a game-theoretic optimal one. In this paper we introduce a new algorithm, UCT, which extends a bandit algorithm for Monte-Carlo search. It is proven that the probability that the algorithm selects the correct move converges to 1. Moreover it is shown empirically that the algorithm converges rather fast even in comparison with alpha-beta search. Experiments in Amazons and Clobber indicate that the UCT algorithm outperforms considerably a plain Monte-Carlo version, and it is competitive against alpha-beta based game programs.},
  file = {/home/shadex91/Zotero/storage/RS2BMRXG/Kocsis et al. - Improved Monte-Carlo Search.pdf},
  language = {en}
}

@article{KombinatorischeSpieltheorie2019,
  title = {{Kombinatorische Spieltheorie}},
  year = {2019},
  month = dec,
  abstract = {Kombinatorische Spieltheorie ist ein von John Horton Conway ca. 1970 begr\"undeter Zweig der Mathematik, der sich mit einer speziellen Klasse von Zwei-Personen-Spielen befasst.
Die Eigenschaften dieser Spiele, die auch als kombinatorische Spiele bezeichnet werden, sind:

Kein Zufallseinfluss.
Es gibt keine f\"ur einen einzelnen Spieler verborgene Information (wie bei Spielkarten). d. h. es liegt perfekte Information vor.
Gezogen wird abwechselnd.
Es gewinnt derjenige Spieler, dem es gelingt, den letzten Zug zu machen.
Jede Partie endet nach einer endlichen Zahl von Z\"ugen.Solche Spiele, zu denen Nim und (nach geringf\"ugigen Regeltransformationen) Go und Schach geh\"oren, er\"offnen besonders dann interessante M\"oglichkeiten der mathematischen Analyse, wenn sie in Komponenten zerfallen, bei denen es keine gegenseitige Beeinflussung der Zugm\"oglichkeiten gibt. Beispiele sind Nim-Haufen und einige sp\"ate Endspielpositionen im Go; auch im Schach lassen sich einige Zugzwang-Positionen bei Bauernendspielen so deuten. Das Zusammensetzen von Positionen wird auch als Addition bezeichnet.
Die mathematische Bedeutung der kombinatorischen Spieltheorie resultiert daraus, dass die Spiele einer Unterklasse als Zahlen gedeutet werden k\"onnen. Dabei lassen sich sowohl ganze als auch reelle und sogar transfinite (d. h. unendlich gro\ss e und unendlich kleine) Zahlen konstruieren, deren Gesamtheit man auch surreale Zahlen nennt. Umgekehrt erscheinen die Spiele der kombinatorischen Spieltheorie als Verallgemeinerung der surrealen Zahlen.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {/home/shadex91/Zotero/storage/V2RDU6H4/index.html},
  journal = {Wikipedia},
  language = {de}
}

@misc{luberWasIstKaggle2020,
  title = {{Was ist Kaggle?}},
  author = {Luber, Stefan},
  year = {2020},
  month = aug,
  abstract = {Kaggle ist eine Online-Plattform f\"ur den Wissensaustausch und Wettbewerbe rund um die Datenanalyse, Machine Learning (ML), Data Mining und Big Data. Zielgruppe der Plattform sind Datenwissenschaftler sowie Unternehmen und Organisationen aus unterschiedlichsten Branchen. Die Mitglieder entwickeln Modelle, Daten nach bestimmten Vorgaben zu analysieren. F\"ur die besten L\"osungen sind in der Regel hohe Geldpreise ausgeschrieben.},
  file = {/home/shadex91/Zotero/storage/BJ6YY5PK/was-ist-kaggle-a-951812.html},
  howpublished = {https://www.bigdata-insider.de/was-ist-kaggle-a-951812/},
  language = {de}
}

@misc{maoUnveilAlphaZero2018,
  title = {{Unveil AlphaZero}},
  author = {Mao, Lei},
  year = {2018},
  month = apr,
  abstract = {Hello Underworld.},
  file = {/home/shadex91/Zotero/storage/JSXRB4IA/Alpha-Zero.html},
  howpublished = {https://leimao.github.io/article/Alpha-Zero/},
  journal = {Lei Mao's Log Book},
  language = {Englisch}
}

@article{mccullochLogicalCalculusIdeas1943,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author = {McCulloch, Warren S. and Pitts, Walter},
  year = {1943},
  month = dec,
  volume = {5},
  pages = {115--133},
  issn = {1522-9602},
  doi = {10.1007/BF02478259},
  abstract = {Because of the ``all-or-none'' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
  file = {/home/shadex91/Zotero/storage/FVVXRNWX/McCulloch and Pitts - 1943 - A logical calculus of the ideas immanent in nervou.pdf},
  journal = {The bulletin of mathematical biophysics},
  language = {en},
  number = {4}
}

@article{mehatCombiningUCTNested2010,
  title = {Combining {{UCT}} and {{Nested Monte Carlo Search}} for {{Single}}-{{Player General Game Playing}}},
  author = {M{\'e}hat, Jean and Cazenave, Tristan},
  year = {2010},
  month = dec,
  volume = {2},
  pages = {271--277},
  issn = {1943-0698},
  doi = {10.1109/TCIAIG.2010.2088123},
  abstract = {Monte Carlo tree search (MCTS) has been recently very successful for game playing, particularly for games where the evaluation of a state is difficult to compute, such as Go or General Games. We compare nested Monte Carlo (NMC) search, upper confidence bounds for trees (UCT-T), UCT with transposition tables (UCT+T), and a simple combination of NMC and UCT+T (MAX) on single-player games of the past General Game Playing (GGP) competitions. We show that transposition tables improve UCT and that MAX is the best of these four algorithms. Using UCT+T, the program Ary won the 2009 GGP competition. MAX and NMC are slight improvements over this 2009 version.},
  file = {/home/shadex91/Zotero/storage/S5TGZS86/Méhat and Cazenave - 2010 - Combining UCT and Nested Monte Carlo Search for Si.pdf;/home/shadex91/Zotero/storage/IAY4N6DM/5604665.html},
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  keywords = {Algorithm design and analysis,Classification algorithms,computer games,Decision trees,Games,General Game Playing (GPP),MAX,Monte Carlo methods,nested Monte Carlo tree search,nested Monte-Carlo search,program Ary,single player general game playing,single-player games,transposition tables,tree searching,UCT+T,upper confidence bounds,upper confidence bounds for trees (UCT)},
  number = {4}
}

@article{Minimax2020,
  title = {Minimax},
  year = {2020},
  month = jun,
  abstract = {Minimax (sometimes MinMax, MM or saddle point) is a decision rule used in artificial intelligence, decision theory, game theory, statistics, and philosophy for minimizing the possible loss for a worst case (maximum loss) scenario.  When dealing with gains, it is referred to as "maximin"\textemdash to maximize the minimum gain.  Originally formulated for two-player zero-sum game theory, covering both the cases where players take alternate moves and those where they make simultaneous moves, it has also been extended to more complex games and to general decision-making in the presence of uncertainty.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {/home/shadex91/Zotero/storage/295BV8HD/index.html},
  journal = {Wikipedia},
  language = {en}
}

@misc{missinglink.aiCompleteGuideArtificial,
  title = {Complete {{Guide}} to {{Artificial Neural Network Concepts}} \& {{Models}}},
  author = {{MissingLink.ai}},
  abstract = {Learn fundamental concepts of neural networks - backpropagation, activation functions, hyperparameters, RNN, CNN and more.},
  file = {/home/shadex91/Zotero/storage/5GP44PLE/complete-guide-artificial-neural-networks.html},
  howpublished = {https://missinglink.ai/guides/neural-network-concepts/complete-guide-artificial-neural-networks/},
  journal = {MissingLink.ai},
  language = {en-US}
}

@misc{missinglink.aiTypesActivationFunctions,
  title = {7 {{Types}} of {{Activation Functions}} in {{Neural Networks}}: {{How}} to {{Choose}}?},
  shorttitle = {7 {{Types}} of {{Activation Functions}} in {{Neural Networks}}},
  author = {{MissingLink.ai}},
  abstract = {Understand the evolution of different types of activation functions in neural network and learn the pros and cons of linear, step, ReLU, PRLeLU, Softmax and Swish.},
  file = {/home/shadex91/Zotero/storage/K596Q2F9/7-types-neural-network-activation-functions-right.html},
  howpublished = {https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/},
  journal = {MissingLink.ai},
  language = {en-US}
}

@article{nielsenNeuralNetworksDeep2015,
  title = {Neural {{Networks}} and {{Deep Learning}}},
  author = {Nielsen, Michael A.},
  year = {2015},
  publisher = {{Determination Press}},
  file = {/home/shadex91/Zotero/storage/SZNM7PDK/neuralnetworksanddeeplearning.com.html},
  language = {en}
}

@article{nielsenNeuralNetworksDeep2015a,
  title = {Neural {{Networks}} and {{Deep Learning}}},
  author = {Nielsen, Michael A.},
  year = {2015},
  publisher = {{Determination Press}},
  file = {/home/shadex91/Zotero/storage/3E6STET5/chap2.html},
  language = {en}
}

@misc{nogueiraMinimaxAlgorithm2006,
  title = {Minimax Algorithm},
  author = {Nogueira, Nuno},
  year = {2006},
  month = dec,
  file = {/home/shadex91/Zotero/storage/IS9PPH98/index.html}
}

@article{openaiDotaLargeScale2019,
  title = {{Dota 2 with Large Scale Deep Reinforcement Learning}},
  author = {OpenAI and Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and J{\'o}zefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Pinto, Henrique Pond{\'e} de Oliveira and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
  year = {2019},
  month = dec,
  abstract = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
  archivePrefix = {arXiv},
  eprint = {1912.06680},
  eprinttype = {arxiv},
  file = {/home/shadex91/Zotero/storage/8MRHJQFJ/OpenAI et al. - 2019 - Dota 2 with Large Scale Deep Reinforcement Learnin.pdf;/home/shadex91/Zotero/storage/WWGKKBTY/1912.html},
  journal = {arXiv:1912.06680 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {Englisch},
  primaryClass = {cs, stat}
}

@misc{paraschivMonteCarloTree2017,
  title = {Monte {{Carlo Tree Search}} for {{Tic}}-{{Tac}}-{{Toe Game}}},
  author = {Paraschiv, Eugen},
  year = {2017},
  month = jun,
  abstract = {Learn Monte Carlo Tree Search (MCTS) algorithm and its applications by exploring implementation for Tic-Tac-Toe game in Java.},
  file = {/home/shadex91/Zotero/storage/67DEF6YY/java-monte-carlo-tree-search.html},
  journal = {Baeldung},
  language = {en-US}
}

@article{PerceptronsBook2020,
  title = {{\emph{Perceptrons}} (Book)},
  year = {2020},
  month = may,
  abstract = {Perceptrons: an introduction to computational geometry is a book written by Marvin Minsky and Seymour Papert and published in 1969. An edition with handwritten corrections and additions was released in the early 1970s. An expanded edition was further published in 1987, containing a chapter dedicated to counter the criticisms made of it in the 1980s.
The main subject of the book is the perceptron, a type of artificial neural network developed in the late 1950s and early 1960s. The book was dedicated to psychologist  Frank Rosenblatt, who in 1957 had published the first model of a "Perceptron". Rosenblatt and Minsky knew each other since adolescence, having studied with a one-year difference at the Bronx High School of Science. They became at one point central figures of a debate inside the AI research community, and are known to have promoted loud discussions in conferences, yet remained friendly.This book is the center of a long-standing controversy in the study of artificial intelligence. It is claimed that pessimistic predictions made by the authors were responsible for a change in the direction of research in AI, concentrating efforts on so-called "symbolic" systems, a line of research that petered out and contributed to the so-called AI winter of the 1980s, when AI's promise was not realized. 
The meat of Perceptrons is a number of mathematical proofs which acknowledge some of the perceptrons' strengths while also showing major limitations. The most important one is related to the computation of some predicates, such as the XOR function, and also the important connectedness predicate. The problem of connectedness is illustrated at the awkwardly colored cover of the book, intended to show how humans themselves have difficulties in computing this predicate.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {/home/shadex91/Zotero/storage/G96DVBCI/index.html},
  journal = {Wikipedia},
  language = {en}
}

@misc{ponsSolvingConnectHow,
  title = {Solving {{Connect}} 4: How to Build a Perfect {{AI}}},
  shorttitle = {Solving {{Connect}} 4},
  author = {Pons, Pascal},
  file = {/home/shadex91/Zotero/storage/HRW6F9IP/blog.gamesolver.org.html},
  howpublished = {http://blog.gamesolver.org/},
  journal = {Solving Connect 4: how to build a perfect AI},
  language = {en}
}

@misc{ponsVierGewinntLoeser,
  title = {{Vier gewinnt L\"oser}},
  author = {Pons, Pascal},
  abstract = {Spiele Vier Gewinnt online gegen einen perfekten Spieler},
  file = {/home/shadex91/Zotero/storage/5542E6DC/connect4.gamesolver.org.html},
  howpublished = {https://connect4.gamesolver.org/de/},
  journal = {Game Solver},
  language = {de}
}

@misc{prasadLessonsImplementingAlphaZero2018,
  title = {Lessons {{From Implementing AlphaZero}}},
  author = {Prasad, Aditya},
  year = {2018},
  month = jul,
  abstract = {DeepMind's AlphaZero publication was a landmark in reinforcement learning (RL) for board game play. The algorithm achieved superhuman\ldots},
  file = {/home/shadex91/Zotero/storage/MN4XFX8M/lessons-from-implementing-alphazero-7e36e9054191.html},
  journal = {Medium},
  language = {en}
}

@book{rashidNeuronaleNetzeSelbst2017,
  title = {{Neuronale Netze selbst programmieren: Ein verst\"andlicher Einstieg mit Python}},
  shorttitle = {{Neuronale Netze selbst programmieren}},
  author = {Rashid, Tariq},
  year = {2017},
  month = apr,
  publisher = {{O'Reilly}},
  address = {{Heidelberg}},
  abstract = {Neuronale Netze sind Schl\"usselelemente des Deep Learning und der K\"unstlichen Intelligenz, die heute zu Erstaunlichem in der Lage sind. Sie sind Grundlage vieler Anwendungen im Alltag wie beispielsweise Spracherkennung, Gesichtserkennung auf Fotos oder die Umwandlung von Sprache in Text. Dennoch verstehen nur wenige, wie neuronale Netze tats\"achlich funktionieren.  Dieses Buch nimmt Sie mit auf eine unterhaltsame Reise, die mit ganz einfachen Ideen beginnt und Ihnen Schritt f\"ur Schritt zeigt, wie neuronale Netze arbeiten:  - Zun\"achst lernen Sie die mathematischen Konzepte kennen, die den neuronalen Netzen zugrunde liegen. Daf\"ur brauchen Sie keine tieferen Mathematikkenntnisse, denn alle mathematischen Ideen werden behutsam und mit vielen Illustrationen und Beispielen erl\"autert. Eine Kurzeinf\"uhrung in die Analysis unterst\"utzt Sie dabei.  - Dann geht es in die Praxis: Nach einer Einf\"uhrung in die popul\"are und leicht zu lernende Programmiersprache Python bauen Sie allm\"ahlich Ihr eigenes neuronales Netz mit Python auf. Sie bringen ihm bei, handgeschriebene Zahlen zu erkennen, bis es eine Performance wie ein professionell entwickeltes Netz erreicht.  - Im n\"achsten Schritt tunen Sie die Leistung Ihres neuronalen Netzes so weit, dass es eine Zahlenerkennung von 98 \% erreicht \textendash{} nur mit einfachen Ideen und simplem Code. Sie testen das Netz mit Ihrer eigenen Handschrift und werfen noch einen Blick in das mysteri\"ose Innere eines neuronalen Netzes.  - Zum Schluss lassen Sie das neuronale Netz auf einem Raspberry Pi Zero laufen.  Tariq Rashid erkl\"art diese schwierige Materie au\ss ergew\"ohnlich klar und verst\"andlich, dadurch werden neuronale Netze f\"ur jeden Interessierten zug\"anglich und praktisch nachvollziehbar.},
  isbn = {978-3-96009-043-4},
  language = {Deutsch},
  translator = {Langenau, Frank}
}

@misc{ronaghanDeepLearningWhich2019,
  title = {Deep {{Learning}}: {{Which Loss}} and {{Activation Functions}} Should {{I}} Use?},
  shorttitle = {Deep {{Learning}}},
  author = {Ronaghan, Stacey},
  year = {2019},
  month = aug,
  abstract = {The purpose of this post is to provide guidance on which combination of final-layer activation function and loss function should be used in\ldots},
  file = {/home/shadex91/Zotero/storage/3MPNN7JV/deep-learning-which-loss-and-activation-functions-should-i-use-ac02f1c56aa8.html},
  journal = {Medium},
  language = {en}
}

@article{rosenblattPerceptronProbabilisticModel1958,
  title = {The {{Perceptron}}: {{A Probabilistic Model}} for {{Information Storage}} and {{Organization}}},
  shorttitle = {The {{Perceptron}}},
  author = {Rosenblatt, Frank},
  year = {1958},
  month = nov,
  volume = {65},
  pages = {386--408},
  abstract = {If we are eventually to understand the capability of higher organisms for perceptual recognition, generalization, recall, and thinking, we must first have answers to three fundamental questions: 1. How is information about the physical world sensed, or detected, by the biological system? 2. In what form is information stored, or remembered? 3. How does information contained in storage, or in memory, influence recognition and behavior? The first of these questions is in the province of sensory physiology, and is the only one for which appreciable understanding has been achieved. This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory. With regard to the second question, two alternative positions have been maintained. The first suggests that storage of sensory information is in the form of coded representations or images, with some sort of one-to-one mapping between the sensory stimulus 1 The development of this theory has been carried out at the Cornell Aeronautical Laboratory, Inc., under the sponsorship of the Office of Naval Research, Contract Nonr-2381(00). This article is primarily'an adaptation of material reported in Ref. IS, which constitutes the first full report on the program.},
  file = {/home/shadex91/Zotero/storage/8MG35S6D/Rosenblatt - The Perceptron A Probabilistic Model for Informat.pdf;/home/shadex91/Zotero/storage/N9GHP476/summary.html},
  journal = {Psychologial Review},
  number = {6}
}

@book{russellArtificialIntelligenceModern2009,
  title = {Artificial {{Intelligence}}: {{A Modern Approach}}},
  shorttitle = {Artificial {{Intelligence}}},
  author = {Russell, Stuart and Norvig, Peter},
  year = {2009},
  month = dec,
  edition = {3 edition},
  publisher = {{Pearson}},
  address = {{Upper Saddle River}},
  abstract = {Artificial Intelligence: A Modern Approach, 3e offers the most comprehensive, up-to-date introduction to the theory and practice of artificial intelligence. Number one in its field, this textbook is ideal for one or two-semester, undergraduate or graduate-level courses in Artificial Intelligence.   Dr. Peter Norvig, contributing Artificial Intelligence author and Professor Sebastian Thrun, a Pearson author are offering a free online course at Stanford University on artificial intelligence.    According to an article in  The New York Times , the course on artificial intelligence is ``one of three being offered experimentally by the Stanford computer science department to extend technology knowledge and skills beyond this elite campus to the entire world.'' One of the other two courses, an introduction to database software, is being taught by Pearson author Dr. Jennifer Widom.     Artificial Intelligence: A Modern Approach, 3e is available to purchase as an eText for your Kindle\texttrademark, NOOK\texttrademark, and the iPhone\textregistered/iPad\textregistered.    To learn more about the course on artificial intelligence, visit http://www.ai-class.com. To read the full New York Times article, click here.},
  isbn = {978-0-13-604259-4},
  language = {English}
}

@article{silverMasteringChessShogi2017,
  title = {{Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm}},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  year = {2017},
  month = dec,
  abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
  archivePrefix = {arXiv},
  eprint = {1712.01815},
  eprinttype = {arxiv},
  file = {/home/shadex91/Zotero/storage/Z43PHL2M/Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a Gene.pdf;/home/shadex91/Zotero/storage/8ZJHUH8J/1712.html},
  journal = {arXiv:1712.01815 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  language = {Englisch},
  primaryClass = {cs}
}

@article{silverMasteringGameGo2016,
  title = {{Mastering the game of Go with deep neural networks and tree search}},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  volume = {529},
  pages = {484--489},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature16961},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  file = {/home/shadex91/Zotero/storage/C3WFLU67/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf;/home/shadex91/Zotero/storage/WQ8PM2CQ/nature16961.html},
  journal = {Nature},
  language = {Englisch},
  number = {7587}
}

@article{silverMasteringGameGo2017,
  title = {{Mastering the game of Go without human knowledge}},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {van den Driessche}, George and Graepel, Thore and Hassabis, Demis},
  year = {2017},
  month = oct,
  volume = {550},
  pages = {354--359},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature24270},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100\textendash 0 against the previously published, champion-defeating AlphaGo.},
  copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  file = {/home/shadex91/Zotero/storage/GANLQK37/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf;/home/shadex91/Zotero/storage/TQU767Y8/nature24270.html},
  journal = {Nature},
  language = {Englisch},
  number = {7676}
}

@article{SoftwareAgent2019,
  title = {{Software-Agent}},
  year = {2019},
  month = jul,
  abstract = {Als Software-Agent (auch Agent oder Softbot) bezeichnet man ein Computerprogramm, das zu gewissem (wohl spezifiziertem) eigenst\"andigem und eigendynamischem (autonomem) Verhalten f\"ahig ist. Das bedeutet, dass abh\"angig von verschiedenen Zust\"anden (Status) ein bestimmter Verarbeitungsvorgang abl\"auft, ohne dass von au\ss en ein weiteres Startsignal gegeben wird oder w\"ahrend des Vorgangs ein \"au\ss erer Steuerungseingriff erfolgt.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {/home/shadex91/Zotero/storage/ATRQEF2V/index.html},
  journal = {Wikipedia},
  language = {de}
}

@book{suttonReinforcementLearningIntroduction2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  file = {/home/shadex91/Zotero/storage/S3Y38LPA/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf},
  isbn = {978-0-262-03924-6},
  keywords = {Reinforcement learning},
  language = {en},
  lccn = {Q325.6 .R45 2018},
  series = {Adaptive Computation and Machine Learning Series}
}

@article{tianELFOpenGoAnalysis2019,
  title = {{ELF OpenGo: An Analysis and Open Reimplementation of AlphaZero}},
  shorttitle = {{ELF OpenGo}},
  author = {Tian, Yuandong and Ma, Jerry and Gong, Qucheng and Sengupta, Shubho and Chen, Zhuoyuan and Pinkerton, James and Zitnick, C. Lawrence},
  year = {2019},
  month = may,
  abstract = {The AlphaGo, AlphaGo Zero, and AlphaZero series of algorithms are remarkable demonstrations of deep reinforcement learning's capabilities, achieving superhuman performance in the complex game of Go with progressively increasing autonomy. However, many obstacles remain in the understanding of and usability of these promising approaches by the research community. Toward elucidating unresolved mysteries and facilitating future research, we propose ELF OpenGo, an open-source reimplementation of the AlphaZero algorithm. ELF OpenGo is the first open-source Go AI to convincingly demonstrate superhuman performance with a perfect (20:0) record against global top professionals. We apply ELF OpenGo to conduct extensive ablation studies, and to identify and analyze numerous interesting phenomena in both the model training and in the gameplay inference procedures. Our code, models, selfplay datasets, and auxiliary data are publicly available.},
  archivePrefix = {arXiv},
  eprint = {1902.04522},
  eprinttype = {arxiv},
  file = {/home/shadex91/Zotero/storage/5XELDB44/Tian et al. - 2019 - ELF OpenGo An Analysis and Open Reimplementation .pdf;/home/shadex91/Zotero/storage/V3UA8GRY/1902.html},
  journal = {arXiv:1902.04522 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {Englisch},
  primaryClass = {cs, stat}
}

@inproceedings{vodopivecEnhancingUpperConfidence2014,
  title = {Enhancing Upper Confidence Bounds for Trees with Temporal Difference Values},
  booktitle = {2014 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}}},
  author = {Vodopivec, Tom and Ster, Branko},
  year = {2014},
  month = aug,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Dortmund, Germany}},
  doi = {10.1109/CIG.2014.6932895},
  abstract = {Upper confidence bounds for trees (UCT) is one of the most popular and generally effective Monte Carlo tree search (MCTS) algorithms. However, in practice it is relatively weak when not aided by additional enhancements. Improving its performance without reducing generality is a current research challenge. We introduce a new domain-independent UCT enhancement based on the theory of reinforcement learning. Our approach estimates state values in the UCT tree by employing temporal difference (TD) learning, which is known to outperform plain Monte Carlo sampling in certain domains. We present three adaptations of the TD(\dbend\dbend\dbend\dbend\dbend\dbend ) algorithm to the UCT's tree policy and backpropagation step. Evaluations on four games (Gomoku, Hex, Connect Four, and Tic Tac Toe) reveal that our approach increases UCT's level of play comparably to the rapid action value estimation (RAVE) enhancement. Furthermore, it proves highly compatible with a modified all moves as first heuristic, where it considerably outperforms RAVE. The findings suggest that integration of TD learning into MCTS deserves further research, which may form a new class of MCTS enhancements.},
  file = {/home/shadex91/Zotero/storage/VTG93JZD/Vodopivec and Ster - 2014 - Enhancing upper confidence bounds for trees with t.pdf},
  isbn = {978-1-4799-3547-5},
  language = {en}
}

@article{vodopivecMonteCarloTree2017,
  title = {On {{Monte Carlo Tree Search}} and {{Reinforcement Learning}}},
  author = {Vodopivec, Tom and Samothrakis, Spyridon and Ster, Branko},
  year = {2017},
  month = dec,
  volume = {60},
  pages = {881--936},
  issn = {1076-9757},
  doi = {10.1613/jair.5507},
  abstract = {Fuelled by successes in Computer Go, Monte Carlo tree search (MCTS) has achieved widespread adoption within the games community. Its links to traditional reinforcement learning (RL) methods have been outlined in the past; however, the use of RL techniques within tree search has not been thoroughly studied yet. In this paper we re-examine in depth this close relation between the two fields; our goal is to improve the cross-awareness between the two communities. We show that a straightforward adaptation of RL semantics within tree search can lead to a wealth of new algorithms, for which the traditional MCTS is only one of the variants. We confirm that planning methods inspired by RL in conjunction with online search demonstrate encouraging results on several classic board games and in arcade video game competitions, where our algorithm recently ranked first. Our study promotes a unified view of learning, planning, and search.},
  file = {/home/shadex91/Zotero/storage/6CXPFXTW/Vodopivec et al. - 2017 - On Monte Carlo Tree Search and Reinforcement Learn.pdf},
  journal = {Journal of Artificial Intelligence Research},
  language = {en}
}


