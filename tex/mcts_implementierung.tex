\section{Implementierung der Monte-Carlo-Baumsuche}
\label{chap:mcts-impl}

Der Quellcode des Projektes befindet sich auf Github: 

In diesem Kapitel werde ich meine Implementierungen der Monte-Carlo-Baumsuche und der ausgewählten Verbesserungen erklären.
Zunächst beschreibe ich kurz, wie das Spielfeld repräsentiert wird und wie die gemeinsame Schnittstelle der verschiedenen Agenten aussieht.
Danach stelle ich die Vergleichs-Algorithmen vor, die verwendet werden, um die korrekte Funktionsweise der Agenten sicherzustellen und um ihre optimalen Parameter zu bestimmen.
Anschließend wird zuerst die reine Monte-Carlo-Baumsuche ohne Verbesserung implementiert, dann wird sie um Transpositionserkennung erweitert.
Die Agenten für Scorebounded MCTS und RAVE MCTS sind ebenfalls Modifikationen der reinen Monte-Carlo-Baumsuche.
Zum Schluss versuche ich die verschiedenen Verbesserungen zu kombinieren.

\subsection{Vier Gewinnt}
\label{chap:viergewinnt-impl}
Die Monte-Carlo-Baumsuche benötigt eine Simulation der Umgebung.
Der Kaggle-Wettbewerb liefert bereits mit dem Python Package \verb|kaggle-environments| eine Spielumgebung für ein parametrisierbares Vier Gewinnt.
Diese Umgebung wird vom Wettbewerb selbst für die Evaluierung der eingereichten Agenten verwendet, ist aber für meine Zwecke unnötig Umfangreich und unbequem zu verwenden.

Deshalb war es zuerst notwendig, eine eigene Implementierung von Vier Gewinnt zu erstellen.
Die Klasse \verb|bachelorarbeit.games.ConnectFour| ist eine Erweiterung der Logik aus dem \verb|connectx.py| Modul von kaggle-environments\footnote{Quelle: \texttt{https://github.com/Kaggle/kaggle-environments/tree/master/kaggle\_environments/envs/connectx}}.

Das Spielfeld wird in einer Liste der Länge \verb|anzahl_spalten * anzahl_zeilen| gespeichert.
Ein leeres Feld wird durch eine 0, Steine von Spieler 1 und 2 durch die Zahlen 1 und respektive 2 repräsentiert.
Index 0 ist  die obere linke Zelle des Spielfeldes, der letzte Index ist die untere rechte Zelle.
Die oberste Zeile des Spielfeldes entspricht den ersten \verb|anzahl_spalten| Werten der Liste.

Diese Repräsentation lässt sich leicht in eine zwei-Dimensionale Darstellung umwandeln.

Die wichtigsten Methoden der Klasse \verb|ConnectFour| sind
\begin{itemize}
	\item \verb|play_move(column)| Setzt einen Stein für den aktuellen Spieler in die angegebene Spalte
	\item \verb|list_moves()| Listet alle legalen Spielzüge
	\item \verb|is_terminal()| Gibt \verb|True| zurück, wenn das Spiel vorbei ist, sonst \verb|False|
	\item \verb|get_reward(player)| Gibt die Belohnung für den angegebenen Spieler zurück.\\1 bei einem Sieg, -1 bei einer Niederlage und sonst 0.
\end{itemize}

Zusätzlich kann sich ein Objekt der Klasse selbst kopieren und kann einen Hash des Spielzustandes erzeugen.
Dieser Hash ist für die Transpositionstabelle nötig.

\subsection{Die Schnittstelle}
\label{chap:schnittstelle}
Jeder Agent erbt von der Klasse \verb|bachelorarbeit.base_players.Player|.
Diese Basisklasse gibt die Schnittstelle des Agenten vor.
Es ist notwendig, dass ein Agent die Methode \verb|get_move(Observation, Configuration) -> int| implementiert.

\verb|Observation| und \verb|Configuration| sind zwei Datenstrukturen der Kaggle Umgebung.
In diesem Projekt sind sie im Modul \verb|bachelorarbeit.games| als Python \verb|DataClass| umgesetzt.
Die \verb|Observation| ist der aktuelle Zustand der Spielumgebung.
Sie enthält das Spielfeld als \verb|Observation.board| im selben Format wie oben beschrieben und das Zeichen (\verb|1| oder \verb|2|) des Spielers der am Zug ist.
\verb|Configuration| enthält die Informationen über die Konfiguration des Spiels selbst.
Dies sind die Anzahl der Spalten \verb|Configuration.columns| und Zeilen \verb|Configuration.rows| des Spielfelds, die Anzahl der, für den Sieg nötigen, Steine in einer Reihe \verb|Configuration.inarow| und ein Zeitlimit für die Ausführung des Agenten.

Im Wettbewerb hat jeder Agent eine Vorbereitungszeit beim ersten Zug von 10 Sekunden und für jeden weiteren Zug 5 Sekunden.
Da allerdings die Leistung der Maschinen, auf denen der Wettbewerb ausgeführt wird, nur schwer eingeschätzt werden kann, benutze ich lokal kein Zeitlimit für meine Agenten.
Stattdessen wird die Baumsuche auf eine bestimmte Anzahl an Iterationen limitiert.

Der Rückgabewert der Methode ist der Zug, den der Agent macht, also eine Zahl zwischen 0 und \verb|Configuration.columns-1|.

\begin{lstlisting}[language=Python,caption=Die Basisklasse aller Agenten.,label={lst:baseplayer}]
class Player(ABC):
    name = "Player"

    @abstractmethod
    def get_move(self, observation: Observation, configuration: Configuration) -> int:
        pass
\end{lstlisting}

\subsection{Vergleichsalgorithmen}
\label{chap:vergleiche-impl}

Die Agenten werden mit einer Reihe verschiedener Algorithmen verglichen.
Der einfachste Vergleich ist gegen einen rein zufälligen Spieler \verb|RandomPlayer|.
Dieser Spieler wählt immer einen zufälligen legalen Zug.
Wenn es einem Agenten nicht gelingt nahezu einhundert Prozent der Spiele gegen den zufälligen Spieler zu gewinnen, so muss etwas an ihrer Implementierung fehlerhaft sein.

\begin{lstlisting}[language=Python,caption=Ein Spieler der zufällige Züge wählt.,label={lst:randomplayer}]
class RandomPlayer(Player):
    name = "RandomPlayer"

    def get_move(self, observation: Observation, configuration: Configuration) -> int:
        return random.choice([c for c in range(configuration.columns) if observation.board[c] == 0])
\end{lstlisting}

Der zweite Vergleich ist gegen einen Algorithmus der auch Flat Monte Carlo, also flache Monte Carlo Suche (Flat MC), genannt wird.
Flat Monte Carlo erzeugt nur einen minimalen Spielbaum.
Er besteht nur aus der Wurzel und ihren direkten Kindknoten.
In jedem Schritt wird eine Aktion gleichmäßig zufällig ausgewählt und das Spiel ab diesem Zustand zufällig zu Ende simuliert.
Das Ergebnis dieser Simulation wird für diese Aktion gespeichert.
Der Agent führt zufällige Iterationen durch, bis ein Zeit oder Schrittlimit erreicht ist.
Am Ende wird die Aktion gewählt, die die höchste durchschnittliche Bewertung hat.

Eine stärkere Variante dieses Spielers wählt die Kindknoten nicht zufällig sondern mit der UCB-Formel (Gleichung~\ref{eqn:UCB1}) aus.

\begin{lstlisting}[language=Python,caption=Flache Monte Carlo Suche.,label={lst:flat-mc}]
class FlatMonteCarlo(Player):
    name = "FlatMonteCarloPlayer"

    def __init__(self,
                 max_steps: int = 1000,
                 exploration_constant: float = 0.8,
                 ucb_selection: bool = True,
                 **kwargs):

        super(FlatMonteCarlo, self).__init__(**kwargs)
        self.max_steps = max_steps
        self.ucb_selection = ucb_selection
        self.c_p = exploration_constant

\end{lstlisting}

Der Flat Monte Carlo Spieler kann durch den Parameter \verb|max_steps| und, wenn die UCB-Kindauswahl benutzt wird, zusätzlich mit dem Parameter \verb|exploration_constant| konfiguriert werden.

Die \texttt{get\_move} Methode simuliert \verb|max_steps| Spiele und wählt den ersten Zug vor der Simulation zufällig oder nach der UCB-Regel.
Der Ergebnis dieser Simulation, aus der Sicht des Agenten, fließt in die durchschnittliche Bewertung $Q(a)$ ein.

\begin{lstlisting}[language=Python]
    def get_move(self,
                 observation: Observation,
                 configuration: Configuration) -> int:

        game = ConnectFour(board=observation.board,
                           rows=configuration.rows,
                           columns=configuration.columns,
                           inarow=configuration.inarow,
                           mark=observation.mark)

        n = defaultdict(int)
        q = defaultdict(float)
        scoring_player = game.get_current_player()

        def ucb(m, N=1):
            return q[m] + self.c_p * math.sqrt(math.log(N) / max(1,n[m]))

        for i in range(1, self.max_steps+1):
            if self.ucb_selection:
                a = max(game.list_moves(), key=lambda m: ucb(m, i))
            else:
                a = random.choice(game.list_moves())

            end_state = simulate_game(game.copy().play_move(a))
            r = end_state.get_reward(scoring_player)
            n[a] += 1
            q[a] += (r - q[a])/n[a]

        return max(n.keys(), key=lambda m: ucb(m))
\end{lstlisting}

Die Spiele werden von der Funktion \verb|simulate_game(ConnectFour)| simuliert.
Sie spielt so lange zufällige Spielzüge, bis ein terminaler Zustand erreicht ist.

\begin{lstlisting}[language=Python]
def simulate_game(game):
    while not game.is_terminal():
        move = random.choice(game.list_moves())
        game.play_move(move)
    return game
\end{lstlisting}

Er wird benutzt, um Vergleichswerte der Spielstärke für die normale Monte-Carlo-Baumsuche zu haben.

\subsection{Normale Monte-Carlo-Baumsuche}
\label{subsec:normale-monte-carlo-baumsuche}

Die Monte-Carlo-Baumsuche ist in zwei Klassen im Modul \verb|bachelorarbeit.mcts| implementiert.
Der Spielbaum besteht aus Knoten der Klasse \verb|Node|.
Jeder Knoten speichert die für den UCT-Algorithmus notwendigen Statistiken $Q(v)$ und $N(v)$ als \verb|average_value| und \verb|number_visits| sowie Verweise zu den Kindknoten und dem Elternknoten.
Die Methoden sind im Programm-Code kurz zusammengefasst.

\begin{lstlisting}[language=Python,caption=Der Knoten der Monte-Carlo-Baumsuche,label={lst:mcts-node}]
class Node:
    def __init__(self, game_state: ConnectFour, parent: Node = None):
        self.average_value = 0  # Q(v)
        self.number_visits = 0  # N(v)
        self.children = {}  # C(v), Kinder des Knotens

        self.parent = parent  # der direkte Elternknoten

        self.game_state = game_state  # der Spielzustand in diesem Knoten
        self.possible_moves = game_state.list_moves()  # Aktionen fuer die noch nicht erforschten Kindknoten
        self.expanded = False  # ob der Knoten vollstaendig expandiert ist

    def Q(self) -> float:
        return self.average_value

    def best_child(self, exploration_constant: float = 1.0) -> "Node":
        """Gibt das Kind mit dem hoechsten UCT-Wert zurueck"""
        n_p = math.log(self.number_visits)

        def UCT(child: Node):
            """
            Berechnet den UCT Wert UCT = Q(v') + C_p * sqrt(ln(N(v))/N(v'))
            :param child: Knoten v'
            :return:
            """
            return child.Q() + exploration_constant * math.sqrt(n_p / child.number_visits)

        _, c = max(self.children.items(), key=lambda entry: UCT(entry[1]))
        return c

    def increment_visit_and_add_reward(self, reward: float):
        """Aktualisiert die Statistik dieses Knotens"""
        self.number_visits += 1
        self.average_value += (reward - self.average_value) / self.number_visits

    def expand_one_child(self) -> "Node":
        """ Waehlt einen zufaelligen Spielzug aus und erzeugt den dazugehoerigen Kindknoten"""

\end{lstlisting}

Der Spieler \verb|MCTSPlayer| implementiert den "Upper Confidence Bound for Trees"-Algorithmus aus Kapitel~\ref{chap:UCT} direkt.\improvement{Der Code muss in mehrere Blöcke aufgeteilt und gekürzt werden.}
Die Methode \verb|get_move| enthält zusätzliche Logik um den Spielbaum von einem Spielzug zum nächsten beizubehalten.
Der eigentliche Algorithmus ist in der Methode \verb|perform_search(root: Node)| implementiert.

\begin{lstlisting}[language=Python,caption=Die MCTSPlayer Klasse,label={lst:mcts-player}]

class MCTSPlayer(Player):
    name = "MCTSPlayer"

    def __init__(self, exploration_constant: float = 1.0,
                 max_steps: int = 1000, keep_tree: bool = False, **kwargs):
        super(MCTSPlayer, self).__init__(**kwargs)

        self.exploration_constant = exploration_constant  # UCT Exploration Konstante Cp
        self.keep_tree = keep_tree  # behalte den Baum zwischen Zuegen
        self.root = None  # die Wurzel des Baumes

    def get_move(self, observation: Observation, conf: Configuration) -> int:
        self.reset(conf)  # Da der Spieler zwischen Zuegen persistent ist, koennen hier Variablen pro Zug zurueckgesetzt werden

        # An dieser Stelle wird der aktuelle Zustand im Baum gesucht, falls der Baum zwischen Zuegen erhalten bleibt
        root = self._restore_root(observation, conf)

        # Wenn keine Wurzel gefunden wurde, wird ein neuer Baum erstellt
        if root is None:
            root_game = ConnectFour(columns=conf.columns, rows=conf.rows, inarow=conf.inarow,
                                    mark=observation.mark, board=observation.board)
            root = self.init_root_node(root_game)

        # Fuehrt die eigentliche Suche aus
        best = self.perform_search(root)

        # Speichert den Wurzelknoten fuer den naechsten Zug
        self._store_root(root.children[best])

        return best

    def perform_search(self, root) -> int:
        while self.has_resources():
            leaf = self.tree_policy(root)
            reward = self.evaluate_game_state(leaf.game_state)
            self.backup(leaf, reward)
        return self.best_move(root)

    def tree_policy(self, root: Node) -> Node:
        current = root
        while not current.game_state.is_terminal():
            if current.is_expanded():
                current = current.best_child(self.exploration_constant)
            else:
                return current.expand_one_child()
        return current

    def evaluate_game_state(self, game_state: ConnectFour) -> float:
        game = game_state.copy()
        # Die Bewertung Geschieht aus Sicht des Spielers, der uns in diesen Zustand gefuehrt hat, darum muss der Spieler
        # geholt werden, der zuletzt gezogen hat, nicht der der gerade an der Reihe ist.
        scoring = game.get_other_player(game.get_current_player())
        while not game.is_terminal():
            game.play_move(random.choice(game.list_moves()))

        return game.get_reward(scoring)

    def backup(self, node: Node, reward: float):
        current = node
        while current is not None:
            current.increment_visit_and_add_reward(reward)
            reward = -reward
            current = current.parent

\end{lstlisting}

Die anpassbaren Parameter der \verb|MCTSPlayer|-Klasse sind die Anzahl der Iterationen pro Spielzug \verb|max_steps|, die Konstante \verb|exploration_constant| $C_p$ und das Beibehalten des Baumes zwischen Zügen.

Die optimalen Parameter werden in Kapitel~\ref{chap:results-mcts} bestimmt.

\subsection{MCTS mit Transpositionen}
\label{subsec:mcts-mit-transpositionen}

Bei der Implementierung der Transpositionen habe ich mich dafür entschieden, die UCT1, UCT2 und UCT3 Algorithmen (Seite~\pageref{eqn:UCT1}) von Childs u.a.\ zu implementieren.
Dafür war es notwendig, dass die Knoten zusätzlich zu ihren eigenen Statistiken $Q(v)$ und $N(v)$ auch die Informationen über die, von ihnen erreichten, Kindknoten $Q(v,a)$ und $N(v,a)$ speichern.\improvement{Der Code muss in mehrere Blöcke aufgeteilt und gekürzt werden.}
Diese zusätzlichen Statistiken werden in der Methode \verb|best_child| verwendet, um UCT1 bzw.\ UCT2 zu implementieren.

\begin{lstlisting}[language=Python,label={lst:transpos-node}]
def normalize(val):
    return (val + 1) / 2

class TranspositionNode(Node):
    def __init__(self, *args, **kwargs):
        super(TranspositionNode, self).__init__( *args, **kwargs)
        self.child_values = defaultdict(float)
        self.child_visits = defaultdict(int)
        self.UCT3_val = 0
        self.sim_reward = 0

    def Qsa(self, move):
        return self.child_values[move]

    def Nsa(self, move):
        return self.child_visits[move]

    def best_child(self, C_p: float = 1.0, uct_method: str = "UCT") -> "Node":
        n_p = math.log(self.number_visits)
        parent = self

        def UCT1(action, _):
            return normalize(parent.Qsa(action)) + C_p * math.sqrt(n_p / parent.Nsa(action))

        def UCT2(action, child):
            return normalize(child.Q()) + C_p * math.sqrt(n_p / parent.Nsa(action))

        def UCT3(action, child):
            return normalize(child.UCT3_val) + C_p * math.sqrt(n_p / parent.Nsa(action))

        def default(_, child):
            return normalize(child.Q()) + C_p * math.sqrt(n_p / child.number_visits)

        selection_method = default
        if uct_method == "UCT1":
            selection_method = UCT1
        elif uct_method == "UCT2":
            selection_method = UCT2
        elif uct_method == "UCT3":
            selection_method = UCT3

        _, c = max(self.children.items(), key=lambda ch: selection_method(*ch))
        return c

    def add_child(self, node: "TranspositionNode", move: int):
        self.children[move] = node
        node.parents.append(self)

    def expand(self, player):
        # Waehle einen zufaelligen naechsten Zustand
        move = random.choice(self.possible_moves)
        self.possible_moves.remove(move)
        if len(self.possible_moves) == 0:
            self.expanded = True

        next_state = self.game_state.copy().play_move(move)

        # Und pruefe ob er bereits in der Transpositionstabelle enthalten ist
        hash_state = hash(next_state)
        if hash_state not in player.transpositions:
            # Wenn nicht wird ein neuer Knoten hinzugefuegt
            player.transpositions[hash_state] = TranspositionNode(game_state=next_state)
        next_node = player.transpositions[hash_state]
        self.add_child(next_node, move)

        return next_node

    def update_QUCT3(self):
        if not self.children:
            self.UCT3_val = self.sim_reward
        else:
            summed = 0
            for a, c in self.children.items():
                c_n = self.child_visits[a]
                c_v = -c.UCT3_val
                summed += c_n * c_v
            self.UCT3_val = (self.sim_reward + summed) / self.number_visits

\end{lstlisting}

Die \verb|MCTSPlayer|-Klasse wurde um ein \verb|dict| Erweitert, das die Transpositionen speichert.
Der Schlüssel dieses Dicts ist der Hash des Spielzustandes und der Wert ist der dazugehörige Knoten.
Wenn in der Expansionsphase in \verb|tree_policy()| ein neuer Knoten erzeugt würde, so wird zunächst der Hash des korrespondierenden Spielzustandes erzeugt, und geprüft ob sich dieser Hash bereits in der Transpositionstabelle befindet.
Wenn er bereits vorhanden ist, so wird der gespeicherte Knoten wiederverwendet und als neues Kind des expandierenden Knotens benutzt.
Ist der Zustand noch nicht in der Transpositionstabelle, so wird ein komplett neuer Knoten erzeugt und im Dict gespeichert.
Da die Baumsuche auf eine maximale Anzahl an Iterationen begrenzt wird, sollte es keine Probleme mit der Größe der Transpositionstabelle geben.
Bei lange laufenden Algorithmen kann es aber nötig sein, alte Knoten aus der Tabelle zu entfernen, um den Speicherverbrauch zu limitieren.

Da der Baum durch die Transpositionen zu einem gerichteten Graph wird, kann jeder Knoten mehrere Elternknoten haben.
Dadurch kann die Belohnung in \verb|backup| nicht mehr über den Elternknoten bis zur Wurzel propagiert werden, da der korrekte Pfad daraus nicht ersichtlich ist.
Um dieses Problem zu lösen, wird in \verb|tree_policy| eine Liste der Knoten, die in der Selektionsphase besucht wurden, erzeugt und diese Liste wird in \verb|backup| rückwärts durchlaufen.\improvement{Der Code muss in mehrere Blöcke aufgeteilt und gekürzt werden.}
In \verb|backup| werden zusätzlich noch die Kind-Statistiken $Q(v,a)$ und $N(v,a)$ der jeweiligen Elternknoten aktualisiert.

\begin{lstlisting}[language=Python,label={lst:transposition-player}]
class TranspositionPlayer(MCTSPlayer):
    name = "Transpositionplayer"

    def __init__(self, uct_method: str = "UCT", **kwargs):
        super(TranspositionPlayer, self).__init__(**kwargs)
        self.transpositions = {}
        self.uct_method = uct_method

    def tree_policy(self, root: TranspositionNode) -> List[TranspositionNode]:
        current = root
        path = [root]  # Der Pfad, der waehrend der Selektion durchlaufen wird
        while not current.game_state.is_terminal():
            if current.is_expanded():
                current = current.best_child(self.exploration_constant, uct_method=self.uct_method)
                path.append(current)
            else:
                # Waehle einen zufaelligen naechsten Zustand
                move, next_state = current.get_random_action_and_state()
                # Und pruefe ob er bereits in der Transpositionstabelle enthalten ist
                hash_state = hash(next_state)
                if hash_state not in self.transpositions:
                    # Wenn nicht wird ein neuer Knoten hinzugefuegt
                    self.transpositions[hash_state] = TranspositionNode(game_state=next_state, parent=current)
                next_node = self.transpositions[hash_state]
                current.add_child(next_node, move)
                path.append(next_node)
                return path
        return path

    def backup(self, path: List[TranspositionNode], reward: float):
        prev = None
        for _node in reversed(path):
            _node.increment_visit_and_add_reward(reward)

            if prev is not None:
                m = _node.find_child_action(prev)
                _node.increment_child_visit_and_add_reward(m, -reward)

            prev = _node
            reward = -reward
\end{lstlisting}

\subsection{Score Bounded MCTS}

Die Hauptveränderungen für die Score Bounded MCTS liegt in den zusätzlichen pessimistischen und optimistischen Grenzen \verb|pess| und \verb|opti| in den Knoten.
In der Methode \verb|best_child| werden diese Grenzen benutzt, um gemäß dem in Kapitel~\ref{chap:scorebounded} beschriebenen Regeln nur die Kindknoten zu besuchen, die das Ergebnis verbessern können.\improvement{Der Code muss in mehrere Blöcke aufgeteilt und gekürzt werden.}
Zusätzlich kann mit den Parametern \verb|cut_delta| und \verb|cut_gamma| die Auswahl der Kinder gelenkt werden.

\begin{lstlisting}[language=Python,label={lst:scorebounded-node}]
class ScoreboundedNode(Node):
    def __init__(self, cut_delta: float = 0.0, cut_gamma: float = 0.0, *args, **kwargs):
        super(ScoreboundedNode, self).__init__(*args, **kwargs)
        self.pess = -1
        self.opti = 1
        self.cut_delta = cut_delta
        self.cut_gamma = cut_gamma

        if self.parent:
            self.is_max_node = not self.parent.is_max_node
        else:
            self.is_max_node = True

    def best_child(self, exploration_constant: float = 1.0) -> "ScoreboundedNode":
        n_p = math.log(self.number_visits)
        parent = self
        gamma = self.cut_gamma
        delta = self.cut_delta

        def score_func(c):
            if parent.is_max_node:
                return c.Q() + gamma * c.pess + delta * c.opti
            else:
                return c.Q() - delta * c.pess - gamma * c.opti

        children = list(self.children.values())
        # Entferne Kindknoten die das Ergebnis nicht verbessern koennen
        for c in children:
            if len(children) > 1:
                if self.is_max_node and c.opti <= self.pess:
                    children.remove(c)
                elif not self.is_max_node and c.pess >= self.opti:
                    children.remove(c)

        c = max(children, key=lambda c: score_func(c) + exploration_constant * math.sqrt(n_p / c.number_visits))
        return c

    def min_pess_child(self):
        c_pess = [c.pess for c in self.children.values()]
        if not self.is_expanded():
            c_pess.append(-1)

        return min(c_pess)

    def max_opti_child(self):
        c_opti = [c.opti for c in self.children.values()]
        if not self.is_expanded():
            c_opti.append(1)

        return max(c_opti)
\end{lstlisting}

Der \verb|ScoreBoundedPlayer| implementiert die Algorithmen zur Aktualisierung der Grenzen (siehe Algorithmus~\ref{algo:prop-scorebound} auf Seite~\pageref{algo:prop-scorebound}).

\begin{lstlisting}[language=Python,label={lst:scorebounded-player}]
class ScoreboundedPlayer(MCTSPlayer):
    name = "ScoreboundedPlayer"

    def __init__(self,
                 cut_delta: float = 0.0,
                 cut_gamma: float = 0.0,
                 *args, **kwargs):
        super(ScoreboundedPlayer, self).__init__(*args, **kwargs)
        self.cut_delta = cut_delta
        self.cut_gamma = cut_gamma

    def prop_pess(self, s: ScoreboundedNode):
        if s.parent:
            n = s.parent
            old_pess = n.pess
            if old_pess < s.pess:
                if n.is_max_node:
                    n.pess = s.pess
                    self.prop_pess(n)
                else:
                    n.pess = n.min_pess_child()
                    if old_pess > n.pess:
                        self.prop_pess(n)

    def prop_opti(self, s: ScoreboundedNode):
        if s.parent:
            n = s.parent
            old_opti = n.opti
            if old_opti > s.opti:
                if n.is_max_node:
                    n.opti = n.max_opti_child()
                    if old_opti > n.opti:
                        self.prop_opti(n)
                else:
                    n.opti = s.opti
                    self.prop_opti(n)

    def backup(self, node: ScoreboundedNode, reward: float):
        if node.game_state.is_terminal():
            bound_score = -reward if node.is_max_node else reward
            node.opti = bound_score
            node.pess = bound_score

            self.prop_pess(node)
            self.prop_opti(node)

        current = node
        while current is not None:
            current.increment_visit_and_add_reward(reward)
            reward = -reward

            current = current.parent
\end{lstlisting}


\subsection{RAVE MCTS}

Der \verb|RaveNode| wurde um die Rave Statistiken $Q_{RAVE}(v)$ und $N_{RAVE}(v)$ erweitert.
In \verb|best_child| werden diese Rave Statistiken mit dem regulären Wert des Knotens $Q(v)$ wie in Gleichung~\ref{eqn:uct-rave} kombiniert.\improvement{Der Code muss in mehrere Blöcke aufgeteilt und gekürzt werden.}
Der Faktor $\beta$ berechnet sich wie in Gleichung~\ref{eqn:beta} aus der Anzahl der normalen Besuche und der Anzahl der RAVE-Updates des Knotens.

\begin{lstlisting}[language=Python,label={lst:rave-node}]

class RaveNode(Node):
    def __init__(self, *args, **kwargs):
        super(RaveNode, self).__init__(*args, **kwargs)
        self.rave_count = 0
        self.rave_score = 0
        self.rave_children: Dict[int, "RaveNode"] = {}

    def beta(self, b: float = 0.0) -> float:
        return self.rave_count / (self.rave_count + self.number_visits + self.rave_count * self.number_visits * b * b)

    def QRave(self) -> float:
        return self.rave_score

    def best_child(self, C_p: float = 1.0, b: float = 0.0) -> Tuple["RaveNode", int]:
        n_p = math.log(self.number_visits)

        def UCT_Rave(child: RaveNode):
            beta = child.beta(b)
            return (1 - beta) * child.Q() + beta * child.QRave() \
                   + C_p * math.sqrt(n_p / child.number_visits)

        m, c = max(self.children.items(), key=lambda c: UCT_Rave(c[1]))
        return c, m

    def expand_one_child_rave(self, moves: List[int]) -> "RaveNode":
        nodeclass = type(self)
        move = random.choice(self.possible_moves)  # waehle zufaelligen Zug
        child_state = self.game_state.copy().play_move(move)
        move_name = child_state.get_move_name(move, played=True)
        moves.append(move_name)
        self.children[move] = nodeclass(game_state=child_state, parent=self)
        self.rave_children[move_name] = self.children[move]
        self.possible_moves.remove(move)
        if len(self.possible_moves) == 0:
            self.expanded = True

        return self.children[move]

    def increment_rave_visit_and_add_reward(self, reward):
        self.rave_count += 1
        self.rave_score += (reward - self.rave_score) / self.rave_count
\end{lstlisting}

Damit die richtigen Knoten aktualisiert werden können, muss jeder Zug eindeutig identifizierbar sein.
Wenn ein Stein in eine Spalte gesetzt wird, wird basierend auf der Anzahl der Steine, die sich bereits in der Spalte befinden, das resultierende Feld idenfiziert.
Die Felder sind von unten Links (0) bis oben rechts (\verb|spalten * zeilen - 1|) nummeriert.
Die eindeutige Bezeichnung eines Zuges bildet sich als \verb|move_name = 10 * index + player| wobei \verb|player| das Symbol des ziehenden Spielers (1 oder 2) ist.
Wenn Spieler 2 zum Beispiel einen Stein in Spalte 6, Zeile 0 (unten rechts) setzt, so hat dieser Zug die Nummer \verb|10 * 6 + 2 = 62|.
Die Klasse \verb|ConnectFour| wurde dafür um die Methode\linebreak[2] \texttt{get\_move\_name(column, played=False)} erweitert.

\begin{lstlisting}[language=Python,label={lst:move-name}]
class ConnectFour:
    def __init__(self):
        # ...
        self.stones_per_column = [0] * self.columns

    # ...
    def play_move(self, column: int):
        # ...
        self.stones_per_column[column] += 1

    def get_move_name(self, column: int, played: bool = False) -> int:
        stones_in_col = self.stones_per_column[column]
        player = self.get_current_player()
        if played:
            stones_in_col -= 1
            player = self.get_other_player(player)
        return 10 * (stones_in_col * self.cols + column) + player
\end{lstlisting}

Der \verb|RavePlayer| hat einen Parameter \verb|b| der in der Berechnung des \verb|beta|-Wertes verwendet wird.
Die \verb|tree_policy|, \verb|evaluate_game_state| und \verb|backup| Methoden wurden für Rave neu geschrieben und führen nun eine Liste der gemachten Spielzüge mit.\improvement{Der Code muss in mehrere Blöcke aufgeteilt und gekürzt werden.}
In \verb|backup| wird diese Liste benutzt, um die Knoten auszuwählen, deren Rave Statistik aktualisiert werden soll.

\begin{lstlisting}[language=Python,label={lst:raveplayer}]
class RavePlayer(MCTSPlayer):
    def __init__(self, b: float = 0.0, *args, **kwargs):
        super(RavePlayer, self).__init__(*args, **kwargs)
        self.b = b

    def tree_policy_rave(self, root: "RaveNode", moves) -> "RaveNode":
        current = root

        while not current.game_state.is_terminal():
            if current.is_expanded():
                current, m = current.best_child(self.exploration_constant, b=self.b)
                # Hole den eindeutigen Namen des Spielzugs und merke ihn
                move_name = current.game_state.get_move_name(m, played=True)
                moves.append(move_name)
            else:
                return current.expand_one_child_rave(moves)

        return current

    def evaluate_game_state_rave(self, game_state: ConnectFour, moves: List[int]) -> float:
        game = game_state.copy()
        scoring = game.get_other_player(game.get_current_player())
        while not game.is_terminal():
            m = random.choice(game.list_moves())
            move_name = game.get_move_name(m)
            moves.append(move_name)  # merke welche Zuege gespielt wurden
            game.play_move(m)

        return game.get_reward(scoring)

    def backup_rave(self, node: RaveNode, reward: float, moves: List[int]):
        move_set = set(moves)
        current = node
        while current is not None:
            current.increment_visit_and_add_reward(reward)

            # Wenn eines der Kinder dieses Knotens ueber einen in der Simulation gemachten Spielzug
            # erreich werden koennte, aktualisiere seine Rave Statistik
            for mov, child in current.rave_children.items():
                if mov in move_set:
                    child.increment_rave_visit_and_add_reward(-reward)

            reward = -reward
            current = current.parent

    def perform_search(self, root):
        while self.has_resources():
            moves = []
            leaf = self.tree_policy_rave(root, moves)
            reward = self.evaluate_game_state_rave(leaf.game_state, moves)
            self.backup_rave(leaf, reward, moves)

        return self.best_move(root)
\end{lstlisting}

\improvement[inline]{Das Kapitel endet sehr abrupt, hier muss noch ein weitere Absatz mindestens hin.}