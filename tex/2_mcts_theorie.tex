\section{Monte-Carlo Baumsuche}

In diesem Kapitel werde ich die theoretischen Hintergründe der Monte-Carlo Baumsuche erklären. Angefangen mit einem kurzen Einstieg in das Reinforcement-Learning und einer Einordnung in das größere Feld des maschinellen Lernens werde ich danach die in dieser Arbeit verwendete Notation erklären und die Monte-Carlo Baumsuche aus Reinforcement-Learning Sicht beschreiben. In diesem Zusammenhang wird auch das Exploration Exploitation Dilemma erklärt.\\
\par
Die Monte-Carlo Baumsuche wurde 2006 von Coulom “erfunden”. Er kombinierte erstmals die schrittweise Erstellung eines Spielbaumes mit Monte-Carlo Simulationen. Spielbäume finden schon seit Jahrzehnten Anwendung in der Spieltheorie und der Entwicklung von Algorithmen für Spiele. 

\subsection{Spielbäume}
Brettspiele wie Vier Gewinnt, Schach und Go lassen sich mit Spielbäumen untersuchen und erklären. Die Knoten eines Spielbaumes sind die (legalen) Zustände des Spiels und die Kanten zwischen den Knoten sind die Aktionen, die ausgeführt werden können um von einem Zustand zu einem anderen Zustand zu wechseln. Die Wurzel des Spielbaumes ist der aktuelle Spielzustand. Hat ein Knoten keine hinausführenden Kanten, so ist er ein Blatt. Blätter sind in der Regel terminale Zustände und haben einen Wert - Sieg, Niederlage oder wenn es das Spiel erlaubt Unentschieden.

\subsection{Die vier Schritte des Algorithmus}
Die Monte-Carlo Baumsuche ist konzeptionell sehr einfach. Die Suche beginnt beim aktuellen Zustand, dem Wurzelknoten des Suchbaumes. Solange noch Ressourcen für die Berechnung vorhanden sind, werden Iterationen der Baumsuche durchgeführt. Nachdem die Ressourcen aufgebraucht sind, wird die beste Aktion, der beste Kindknoten, ausgewählt.\\
Eine einzelne Iteration der Baumsuche besteht aus den vier Schritten
\begin{itemize}
	\item Selektion
	\item Expansion
	\item Simulation
	\item Aktualisierung
\end{itemize}

Ausgehend von der Wurzel wird der Suchbaum hinabgestiegen bis entweder ein terminaler Knoten, oder ein Knoten, der noch nicht vollständig expandiert ist, erreicht wurde. Das heißt, es wurden noch nicht alle Kinder dieses Knotens besucht. In jedem Knoten entscheidet die Tree policy, welcher Knoten als nächstes besucht werden soll.
\par
Wenn ein nicht vollständig expandierter Knoten erreicht wurde, so wird dieser expandiert, indem ein noch nicht besuchtes Kind dem Suchbaum hinzugefügt wird, und mit diesem Knoten verbunden wird.
\par
Danach wird von diesem Kind eine Simulation durchgeführt. In der Simulationsphase werden von der Default policy so lange Aktionen gewählt, bis das Spiel einen Endzustand erreicht hat. Das Ergebnis dieser Simulation wird danach verwendet, um die Werte der Knoten im Spielbaum zu aktualisieren.\\
\par
Die Monte-Carlo Baumsuche erzeugt einen asymmetrischen Spielbaum. Knoten, die eine gute Bewertung haben werden häufiger besucht und der Baum in dieser Richtung mehr erweitert.
\subsection{Tree policy}
In jedem Knoten muss das Exploration Exploitation Dilemma gelöst werden. Der tatsächliche Wert eines Kindknotens ist nicht bekannt. Durch die Simulationen kann die Bewertung nur geschätzt werden. Der Algorithmus muss also eine Entscheidung mit unvollständigen Informationen treffen. Werden die Knoten gewählt, die in der Vergangenheit gute Ergebnisse geliefert haben oder werden Knoten ausgewählt, die mit den aktuellen Informationen zwar schlechter bewertet sind, deren geschätzter Wert aber noch sehr unsicher ist?\\
\par
Die Tree policy versucht dieses Problem zu lösen. Das Problem gehört zur Klasse der Banditen-Probleme. Der Name stammt von den Slot-Maschinen, auch “einarmige Banditen” genannt, in Spielkasinos. Jeder “Arm” hat eine Wahrscheinlichkeit p einen Gewinn R auszuschütten, wenn er gezogen wird. In den theoretischen Banditen Problemen ist diese Wahrscheinlichkeit fest und dem Spieler unbekannt. Wie entscheidet der Spieler nun, welcher der beste Arm ist? Der naive Ansatz probiert zunächst jeden Arm einmal aus und merkt sich für jeden Arm die erhaltene Belohnung. Dann nimmt man immer den Arm mit der besten durchschnittlichen Belohnung, man ist gierig (engl. greedy).
\paragraph{Beispiel eines Banditenproblems:}
Bandit 1 (B1) gibt beim ersten Zug eine Belohnung von 0 und Bandit 2 (B2) eine Belohnung von 10. Die durchschnittlichen Belohnungen sind also entsprechend 0 und 10. Ein greedy Agent wählt jetzt so lange B2 bis die Bewertung von B2 geringer ist, als die von B1. Da die minimale Belohnung allerdings 0 ist, wird der Wert nie auf 0 sinken. Der Agent würde also immer den suboptimalen Arm wählen. Wir müssen ihn dazu bringen zu erkunden.
\paragraph{$\epsilon$-greedy}
$\epsilon$-greedy ist ein häufig gewählter Ansatz um dieses Problem zu lösen. Ein $\epsilon$-greedy Algorithmus verhält sich greedy mit einer Wahrscheinlichkeit von $\epsilon$ und zufällig mit $1-\epsilon$. Die zufällige Auswahl ist Einheitlich aus allen vorhandenen Möglichkeiten, auch der greedy Option. Das bedeutet über eine Laufzeit von $t$ Zeiteinheiten wird der Agent mindestens $(\epsilon + (1-\epsilon)/k) * t$ mal die greedy Option wählen und $((1-\epsilon) - (1-\epsilon)/k) * t$ eine rein zufällige andere.\\
\paragraph{Regret}
Unter Regret versteht man den Verlust, der dadurch entsteht, dass nicht die optimale Aktion in einem Zustand ausgewählt wird. Der Wert der optimalen Aktion $v^*$ ist definiert als $v^* = q(r | a)$. Der Regret $L_t$ ist der kumulierte Verlust über die gesamte Laufzeit des Agenten
\begin{equation}
L_t = \sum_{i}^{t}v^* - q(a_i)
\end{equation}
Bei der Betrachtung des Regrets ist nicht der absolute Wert entscheidend, sondern sein Verhalten mit steigendem n. Deshalb ist die Big-O Notation nützlich um über das Wachstum des Regrets zu reden. Es wurde gezeigt, dass der Regret nicht langsamer als $O(\log n)$ wachsen kann. Ein epsilon-greedy Algorithmus hat einen linear wachsenden Regret $O(n)$.

\paragraph{Upper confidence bound}
Der Upper Confidence Bound Algorithmus versucht die Exploration von der Anzahl der durchgeführten Simulationen abhängig zu machen. Zur durchschnittlichen Bewertung eines Armes $Q(a_t)$ kommt noch ein Explorations-Term $U(a_t)$ der die Unsicherheit in der Genauigkeit des geschätzten Wertes angibt. Der Term wird so gewählt, dass $q(a_t) \le Q(a_t) + U(a_t), \forall a_t$. Damit erhalten wir für jede Aktion $a_t\in A$ den optimistisch geschätzten Wert, der aufgrund unserer Unsicherheit höher liegen muss, als der tatsächliche Wert q(at).\\
Die UCB Formel wurde von XXX entwickelt.
\begin{equation}
UCB(a_t) = Q(a_t) + U(a_t)
\end{equation}
mit
\begin{equation}
Q(a_t) = \textrm{durchschnittliche Belohnung erhalten wenn at gewählt}
\end{equation}
und 
\begin{equation}
U(a_t) = \sqrt{
	\frac{2\log(n)}{N(a_t)}
}
\end{equation}
 was häufig allgemeiner als $U(a_t) = C_p \times \sqrt{
 	\frac{\log(n)}
 	{N(a_t)}
}$ geschrieben wird
wobei $n$ = die Anzahl aller Simulationen und $N(a_t)$ = die Anzahl der Simulationen, in denen at gewählt wurde, ist.
Es wurde gezeigt, dass der Regret von UCB logarithmisch steigt.

\paragraph{UCT Tree Policy}
Die UCT Tree Policy ist die gängigste Tree Policy für die Monte-Carlo Baumsuche. UCT bedeutet Upper-confidence Bound for Trees und ist eine Adaption des UCB algorithmus (siehe Kap. X.X) für die Monte-Carlo Baumsuche.
\begin{equation}
UCT(s_t,a_t) = Q(s_t, a_t) + C_p \times \sqrt{
	\frac{\log(N(s_t))}
	{N(s_t,a_t)}
}
\end{equation}


Wobei $Q(s_t,a_t)$ der durchschnittliche Wert eines Knotens ist, $C_p$ ist die UCB Explorations-Konstante, $N(s_t)$ ist die Anzahl der Besuche des Knotens $s_t$ zum Zeitpunkt $t$ und $N(s_t,a_t)$ ist die Anzahl wie häufig Aktion $a_t$ im Zustand $s_t$ gewählt wurde.

\subsection{Default policy}
Die Default Policy wird verwendet, um den Simulationsschritt der Baumsuche zu steuern. Die normale Policy wählt aus den möglichen Aktionen mit gleicher Wahrscheinlichkeit bis das Spiel zu Ende ist. Einige Personen haben gezeigt, dass eine Verbesserung der Default Policy eine beachtliche Steigerung der Effizienz der Baumsuche bewirken kann, andere haben aber gezeigt, dass in bestimmten Fällen eine zufällige default Policy die beste Wahl ist. Insbesondere, weil eine komplexere default Policy zu einer erheblichen Steigerung des Rechenaufwands pro Simulation führen kann.
\par
Silver et al. haben in ihrem Paper über AlphaGo gezeigt, dass das komplette Ersetzen des Simulationsschrittes durch eine Auswertung durch ein neuronales Netz die Fähigkeiten eines Spielers basierend auf der Monte-Carlo Baumsuche im Brettspiel Go auf übermenschliches Niveau anheben kann.

