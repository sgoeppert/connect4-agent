\section{Kaggle Spielumgebung}

Die Spielumgebung die von Kaggle zur Verfügung gestellt wird, orientiert sich in ihrer Programmierschnittstelle an der des OpenAI Gym. Das OpenAI Gym wurde entwickelt, um Forschung im Bereich des Reinforcement Learnings voranzutreiben. Insbesondere der Bedarf nach besseren Benchmarks und einheitlichen Umgebungen in veröffentlichten Papern waren ein Anreiz dafür.
\par
Während andere Bereiche des maschinellen Lernens Benchmarks wie zum Beispiel ImageNet haben, gibt es kein äquivalent für reinforcement learning. Außerdem wird die Reproduzierbarkeit von veröffentlichten Forschungsergebnissen dadurch erschwert, dass es subtile Unterschiede in den verwendeten Umgebungen und Problemdefinitionen gibt, die die Schwierigkeit der Aufgabe drastisch verändern können.
\par
Durch eine einheitliche Schnittstelle, die es leicht macht verschiedenste Umgebungen zu erzeugen und benutzen, wird die Entwicklung neuer Reinforcement learning Algorithmen deutlich vereinheitlicht.

\subsection{Inhalt der Spielumgebung}
Das Python Package \texttt{kaggle\_environments}\footnote{https://github.com/Kaggle/kaggle-environments} liefert einige Spielumgebungen wie zum Beispiel TicTacToe, ConnectX und Halite mit einheitlicher Schnittstelle. Durch ein “make(‘ConnectX’)” wird ein Objekt der Spielumgebung erzeugt.
\par
Mit env.run([agent1, agent2]) kann das Environment ausgeführt werden, bis es terminiert. Die Methode gibt ein Array aller Zustände von Anfang bis Ende und die zugehörigen Belohnungen für beide Spieler zurück. 
\par
Mit env.render(mode) wird das Environment dargestellt. Der Mode gibt dabei an, wie die Darstellung geschieht. Dies kann z.B. interaktiv in einem ipython Notebook oder statisch auf einem Terminal geschehen.
\par
Durch env.train([gegner, None]) kann eine Trainingsumgebung wie das OpenAI Gym erstellt werden. Durch das Keyword None wird angegeben, als welcher Spieler trainiert werden soll. Ein so erzeugter “trainer” kann dann genauso verwendet werden, wie es mit einem OpenAI Gym möglich ist. Durch reset() wird der Trainer initialisiert und zurückgesetzt, außerdem wird so die initiale Observation (Beobachtung) erstellt (siehe Kap. 3.2). Diese Observation wird vom zu trainierenden Agenten verwendet, um eine Entscheidung über die gewählte Aktion zu treffen. Durch Aufruf der Methode trainer.step(action) wird die Aktion durch die Umgebung verarbeitet und die nächste Observation, Belohnung, Spielende und weitere Informationen werden zurückgegeben. trainer.step() kann in einer Schleife aufgerufen werden, bis das Spielende erreicht ist.

\subsection{Repräsentation des Spielfeldes}
Die Observation der ConnectX Umgebung enthält zwei Elemente, das Spielfeld und die Markierung des ziehenden Spielers. Das Spielfeld wird als ein-dimensionales Array dargestellt. Eine 0 markiert einen freien Platz und eine Eins oder Zwei einen Spielstein des jeweiligen Spielers. Die Reihenfolge der Elemente im Array ergibt sich aus einer Nummerierung der Felder im Spielbrett von oben links nach unten rechts. Das heißt die erste(oberste) Zeile des Spielbretts (7 Felder im klassischen Spielbrett) sind auch gleich die ersten Felder in der Observation. Die letzte Zeile des Spielbretts sind die letzten Felder der Observation.\\
\par
Diese Darstellung lässt sich leicht in eine zwei-dimensionale Darstellung umwandeln, um darauf Algorithmen durchzuführen, die die räumlichen Eigenschaften des Spieles ausnutzen.

